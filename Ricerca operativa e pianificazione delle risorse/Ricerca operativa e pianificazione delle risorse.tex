\documentclass[12pt]{article}

\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage[utf8]{inputenc}
\usepackage{changepage}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{biblatex}

\lstset{
  language=Python,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  tabsize=4,
  basicstyle=\ttfamily,
  columns=fullflexible,
  keepspaces,
}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\newenvironment{para}{\begin{adjustwidth}{13mm}{}}{\end{adjustwidth}}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\newcommand{\tabitem}{\llap{\textbullet}}
\newcommand{\Hsquare}{%
\text{\fboxsep=-.2pt\fbox{\rule{0pt}{1ex}\rule{1ex}{0pt}}}%
}

\newtheorem{Definizione}{Definizione}[subsection]
\newtheorem{Lemma}{Lemma}[subsection]
\newtheorem{Teorema/Definizione}{Teorema/Definizione}[subsection]
\newtheorem{Corollario}{Corollario}[subsection]
\newtheorem{Teorema}{Teorema}[subsection]
\newtheorem{Proposizione}{Proposizione}[subsection]
\newtheorem{Notazione}{Notazione}[subsection]
\newtheorem{Commento}{Commento}[subsection]
\newtheorem{Dimostrazione}{Dimostrazione}[subsection]
\newtheorem{Osservazione}{Osservazione}[subsection]
\newtheorem{Nota}{Nota}[subsection]

\title{Ricerca operativa e pianificazione delle risorse}
\author{spitfire}
\date{A.A. 2023-2024}
\begin{document}
\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{Images/Logo scienze bicocca.png}
\end{figure}

\vspace{10cm}
\date{A.A. 2024-2025}


\maketitle

\newpage

\tableofcontents
\newpage

\section{Prerequisiti di Algebra Lineare}
\subsection{Matrici e vettori}
Una matrice è una tabella contenente numeri.
Se la tabella è costituita da $m$ righe e $n$ colonne si parla
di una matrice  $m \times n$. 
Una matrice viene detta \textbf{matrice quadrata} se il numero di righe
e colonne coincidono. \newline
Una matrice $1 \times m$ viene detto \textbf{vettore riga m-dimensionale} \newline
Una matrice $m \times 1$ viene detto \textbf{vettore colonna m-dimensionale}. \newline
La notazione maggiormente utilizzata per indicare una matrice è
$$A = [a_{ij}]$$
Con $a_{ij}$ elemento generico della i-esima riga e j-esima colonna della matrice $A$.
Se $A = [a_{ij}]$ è una matrice $m \times n$, la matrice $n \times m$
$$A^T=[a_{ij}]$$
viene detta \textbf{matrice trasposta} della matrice $A$.

Se $A = [a_{ik}]$ è una matrice $m \times p$ e $B = [b_{kj}]$ è una matrice $p \times n$ la loro
\textbf{matrice prodotto} è $m \times n$ e definita come:
$$A \cdot B = C = [c_{ij}] \; con \; c_{ij} = \sum_{k = 1}^{p} a_{ik} \cdot b_{kj}$$
Date due matrici $m \times n, A = [a_{ij}]$ e $B = [b_{ij}]$, la loro \textbf{matrice somma} è definita come segue:
$$A+B=C=[c_{ij}] \; con \; c_{ij} = a_{ij} + b_{ij}$$
La \textbf{moltiplicazione} di una \textbf{matrice A per una costante $\alpha$} fornisce come risultato quanto segue:
$$\alpha \cdot A = [\alpha \cdot a_{ij}]$$
Questa moltiplicazione è \textbf{commutativa}. \newline
Siano $v_1, v_2, ..., v_n$ n vettori, riga o colonna; essi vengono detti
\textbf{linearmente indipendenti} tra loro se, prendendo $n$ coefficienti $a_1, a_2, ..., a_n$ la seguente uguaglianza
$$a_1 \cdot v_1 + a_2 \cdot v_2 + ... + a_n \cdot v_n = 0$$
risulta verificata solo se $a_1 = a_2 = ... = a_n = 0$. \newline
Al contrario, se esistono coefficienti $a_1, a_2, ..., a_n$ non tutti nulli per cui
$$a_1 \cdot v_1 + a_2 \cdot v_2 + ... + a_n \cdot v_n = 0$$
i vettori $v_1, v_2, ..., v_n$ sono detti \textbf{linearmente dipendenti}. \newline
Un insieme di $n$ vettori ad $n$ dimensioni linearmente indipendenti costituisce una \textbf{base per uno spazio a n dimensioni}.
Se un insieme di vettori $v_1, v_2, ..., v_n$ costituisce una base per uno spazio ad $n$ dimensioni, allora ogni vettore $x$ che appartiene
a quello spazio è \textbf{combinazione lineare dei vettori della base}. \newline
Una matrice quadrata $m \times m$ si dice \textbf{matrice singolare} se l'insieme degli $m$ vettori riga (o colonna), ottenuti considerando
ogni riga (o colonna) come un vettore, è \textbf{linearmente dipendenti}.
Se, viceversa, l'insieme degli $m$ vettori è linearmente indipendente, la matrice si dice \textbf{matrice non singolare}. \newline
Una matrice quadrata $A = [a_{ij}]$ con $a_{ij} = 0$ per ogni $i \neq j$ viene detta \textbf{matrice diagonale}. \newline
La matrice diagonale $A = [a_{ij}]$, con $a_{ii} = 1$ per ogni $i$ viene detta \textbf{matrice identità}, solitamente indicata con $I$.
Se $A$ NON è una matrice singolare, allora esiste una matrice $A^{-1}$ detta \textbf{matrice inversa} della matrice $A$, tale per cui vale la
seguente relazione di uguaglianza:
$$A \cdot A^{-1} = A^{-1} \cdot A = I$$
Il \textbf{determinante} di una matrice quadrata $A$ si indica con $det(A)$ ed è un numero (esiste solo per matrici quadrate), nel caso
specifico di una matrice $2 \times 2$ si definisce come segue:
$$det(A) = det\begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
\end{pmatrix} = a_{11} \cdot a_{22} - a_{12} \cdot a_{21}$$
Il determinante di una matrice quadrata $A$ $m \times m$ si ottiene utilizzando la seguente regola ricorsiva, detta \textbf{formula di Laplace}:
Se $A_{ij}$ è la matrice $(m-1) \times (m-1)$, ottenuta togliendo la i-esima riga e la j-esima colonna di A, il determinante di A risulta:
$$det(A) = \sum_{j=1}^{m} (-1)^{i+j} \cdot a_{ij} \cdot det(A_{ij}) \; (formula \; per \; righe)$$
$$det(A) = \sum_{i=1}^{m} (-1)^{i+j} \cdot a_{ij} \cdot det(A_{ij}) \; (formula \; per \; colonne)$$
Se la matrice è singolare, allora $det(A) = 0$. \newline
Una matrice quadrata $A$ ammette inversa se e solo se non è singolare.
\subsection{Equazioni lineari}
Un' \textbf{equazione lineare} nelle variabili $x_1, x_2, ..., x_n$ è un'equazione nella seguente forma:
$$a_1 \cdot x_1 + a_2 \cdot x_2 + ... + a_n \cdot x_n = b$$
dove $a_1, a_2, ..., a_n$ e $b$ sono delle costanti.
Si dice \textbf{soluzione dell'equazione} un qualsiasi vettore $|y_1, y_2, ..., y_n| \in \mathbb{R}^n$ tale che:
$$a_1 \cdot y_1 + a_2 \cdot y_2 + ... + a_n \cdot y_n = b$$
Un \textbf{sistema di m equazioni lineari in n variabili} è definito come segue:
$$\begin{cases}
    a_{11} \cdot x_1 + a_{12} \cdot x_2 + ... + a_{1n} \cdot x_n = b_1 \\
    a_{21} \cdot x_1 + a_{22} \cdot x_2 + ... + a_{2n} \cdot x_n = b_2 \\
    ... \\
    a_{m1} \cdot x_1 + a_{m2} \cdot x_2 + ... + a_{mn} \cdot x_n = b_m
\end{cases}$$
dove $a_{ij}$ e $b_{j}$, $i = 1,...,n$; $j = 1,...,m$ sono costanti.
Una \textbf{soluzione del sistema lineare} è un qualsiasi vettore $|y_1, y_2, ..., y_n| \in \mathbb{R}^n$ tale che le $m$ equazioni
del sistema lineare siano contemporaneamente soddisfatte.
Trovare le soluzioni del sistema lineare equivale a individuare il punto di intersezione tra le sue equazioni, ammesso che un tale punto esista. \newline
Un sistema di equazioni lineari può essere:
\begin{itemize}
    \item \textbf{Consistente}: se ammette almeno una soluzione, in caso contrario viene detto \textbf{inconsistente}
    \item \textbf{Determinato}: se costituito da un numero di equazioni uguale al numero di incognite $m = n$. Un tale sistema ha \textbf{una sola soluzione}
    \item \textbf{Sovradeterminato}: se costituito da più equazione che incognite $m>n$. Un tale sistema è spesso, ma non sempre, inconsistente
    \item \textbf{Sottodeterminato}: se costituito da meno equazioni che incognite $m<n$. Un tale sistema ammette infinite soluzioni
\end{itemize}
Consideriamo la forma matriciale del sistema costituito da $m$ equazioni lineari in $n$ incognite
$$A \cdot x = b$$
dove
\begin{itemize}
    \item $A$ è una matrice $m \times n$ (nota)
    \item $x$ è un vettore colonna in $n$ dimensioni (incognito)
    \item $b$ è un vettore colonna in $m$ dimensioni (noto)
\end{itemize}
Si definisce \textbf{rango della matrice A} come segue:
\begin{itemize}
    \item \textbf{Rango di riga}: numero massimo di righe linearmente indipendenti
    \item \textbf{Rango di colonna}: numero massimo di colonne linearmente indipendenti
\end{itemize}
Se $rango \; di \; riga = rango \; di \; colonna$ allora $rk(A) \leq min(m, n)$ \newline
Se $rk(A) = min (m, n)$, allora la matrice A viene detta \textbf{a rango pieno}. \newline
Data la matrice dei coefficienti $A$, si dice \textbf{matrice aumentata} la matrice $C = A,b$ ottenuta
dalla matrice $A$ aggiungendo come colonna aggiuntiva il vettore dei termini noti $b$.
Avremo quanto segue:
\begin{itemize}
    \item $rk(C) > rk(A)$: Il sistema lineare non ammette soluzione
    \item $rk(C) = rk(A)$: il sistema lineare ammette soluzione
\end{itemize}
Assumiamo $rk(C) = rk(A)$, allora:
\begin{itemize}
    \item Caso $m \geq n$
    \begin{itemize}
        \item Se $rk(A) = n$, allora il sistema ha una soluzione unica
        \item Se $rk(A) < n$, allora il sistema ha infinite soluzioni
    \end{itemize}
    \item Caso $m < n$
    \begin{itemize}
        \item Se $rk(A) \leq m$, allora il sistema ha infinite soluzioni
    \end{itemize}
\end{itemize}
Come si risolve un sistema di equazioni lineari? Abbiamo due metodi:
\subsubsection{Metodo di eliminazione}
Procediamo come segue:
\begin{enumerate}
    \item Selezionare una variabile, e risolvere una delle equazioni rispetto ad essa e eliminare
    la variabile in questione dalle altre equazioni
    \item Tralasciare l'equazione utilizzata nel passo di eliminazione e tornare al passo 1)
    \item Applicare il processo di \textbf{Back-walk substitution}: dall'ultima equazione, tornare indietro e risolvere le restanti
\end{enumerate}
\subsubsection{Metodo di eliminazione di Gauss}
Il metodo di eliminazione di Gauss è un metodo di eliminazione che utilizza solo le operazioni elementari su matrici, cioé:
\begin{itemize}
    \item Moltiplicare una riga per uno scalare non nullo
    \item Sommare una riga moltiplicata per uno scalare non nullo con un'altra riga
    \item Permutare le righe 
\end{itemize}
\begin{Teorema}
    Applicare operazioni elementari a un sistema di equazioni lineari non cambia l'insieme delle sue soluzioni.
\end{Teorema}
\section{Prerequisiti di Analisi Matematica}
\subsection{Funzioni di una variabile}
Si dice \textbf{funzione} una terna $(A, B, f)$ con:
\begin{itemize}
    \item $A, B$ due insiemi non vuoti
    \item $f$ una legge che ad ogni elemento $x \in A$ associa uno ed uno solo elemento $f(x) \in B$
\end{itemize}
dove:
\begin{itemize}
    \item $A$ è detto dominio della funzione $f$, anche indicato con $dom(f)$
    \item $B$ è detto codominio della funzione $f$
    \item Scriviamo $f: A \rightarrow B$ e $x \in dom(f) \rightarrow f(x)$, per indicare la legge che alla variabile indipendente $x$ associa la sua immagine $f(x)$
\end{itemize}
Data una funzione $f: A \rightarrow B$, se esiste, finito o meno, il limite:
$$\lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h} = \frac{f(x) - f(x_0)}{x - x_0}$$
esso viene chiamato \textbf{derivata della funzione f nel punto $x_0$} e viene indicato con
$$f'(x_0) = \frac{d}{dx}f(x_0)$$
Se $f'(x_0) \in \mathbb{R}$, allora $f$ si dice derivabile in $x_0$. \newline
\newpage
Riportiamo le derivate elementari:
\begin{itemize}
    \item Se $f(x) = c, \forall x \in \mathbb{R}$ allora $f'(x) = 0, \forall x \in \mathbb{R}$
    \item Se $f(x) = x^n, n \in \mathbb{N}, n \geq 2$ allora $f'(x) = n \cdot x^{n-1}, \forall x \in \mathbb{R}$
    \item Se $f(x) = \frac{1}{x}, \forall x \in \mathbb{R}^+$ allora $f'(x) = -\frac{1}{x^2}, \forall x \in \mathbb{R}^+$
    \item Se $f(x) = log(x), x \in \mathbb{R}^+$ allora $f'(x) = \frac{1}{x}, \forall x \in \mathbb{R}^+$
\end{itemize}
Data una funzione $f: \mathbb{R} \rightarrow \mathbb{R}$ e un punto $x_0 \in \mathbb{R}$, allora
\begin{itemize}
    \item $f$ derivabile in $x_0 \Rightarrow f$ continua in $x_0$
    \item $f$ continua in $x_0 \not\Rightarrow f$ derivabile in $x_0$
\end{itemize}
Se $f, g: \mathbb{R} \rightarrow \mathbb{R}$ sono derivabili in $x_0 \in \mathbb{R}$, allora
\begin{itemize}
    \item $\forall c \in \mathbb{R}$, la funzione $c \cdot f$ è derivabile in $x_0$ e $(c \cdot f)'(x_0) = c \cdot f'(x_0)$
    \item La funzione $f + g$ è derivabile in $x_0$ e $(f+g)'(x_0) = f'(x_0) + g'(x_0)$
\end{itemize}
Se $f, g: \mathbb{R} \rightarrow \mathbb{R}$ sono derivabili in $x_0 \in \mathbb{R}$, allora anche la funzione $f \cdot g$ è derivabile
in $x_0$ e si ha quanto segue
$$(f \cdot g)'(x_0) = f'(x_0) \cdot g(x_0) + f(x_0) \cdot g'(x_0)$$
Date due funzioni $f, g: \mathbb{R} \rightarrow \mathbb{R}$, con $f$ derivabile in $x_0 \in \mathbb{R}$ e $g$ derivabile in
$f(x_0)$, allora $g \circ f$ è derivabile in $x_0$ e si ha quanto segue:
$$(g \circ f)'(x_0) = g'(f(x_0)) \cdot f'(x_0)$$
La derivata della \textbf{derivata prima $f'$} in $x_0 \in \mathbb{R}$ viene detta \textbf{derivata seconda} e indicata come $f''(x_0)$. \newline
La derivata è il \textbf{coefficiente angolare} della retta tangente alla funzione nel punto di derivazione $x_0$. \newline
Data una funzione $f(x)$ definita su un intervallo chiuso $[a,b]$ diremo che la funzione è:
\begin{itemize}
    \item \textbf{Crescente}: nell'intervallo $[a,b]$ quando per ogni coppia di punti $x_1, x_2 \in [a,b]$ con $x_1 < x_2$ risulta che $f(x_1) < f(x_2)$
    \item \textbf{Decrescente}: nell'intervallo $[a,b]$ quando per ogni coppia di punti $x_1, x_2 \in [a,b]$ con $x_1 < x_2$ risulta che $f(x_1) > f(x_2)$
\end{itemize}
Per determinare se la funzione $f:[a,b] \rightarrow \mathbb{R}$ sia crescente o decrescente in un punto $x_0 \in [a,b]$ è possibile ricorrere alla valutazione della sua derivata
nel punto $x_0$, infatti:
\begin{itemize}
    \item Se $f'(x_0) >0$ allora è crescente nel punto considerato $x_0$
    \item Se $f'(x_0) <0$ allora la funzione è decrescente nel punto considerato $x_0$
\end{itemize}
Una funzione $f:[a,b] -> \mathbb{R}$ si dice \textbf{convessa} se $\forall x_1, x_2 \in [a,b]$ con $x_1 < x_2$ vale la seguente relazione
$$f(x) \leq f(x_1) + \frac{f(x_2) - f(x_1)}{x_2 - x_1} \cdot (x - x_1) \; \forall x \in [a,b]$$
\textbf{strettamente convessa} se:
$$f(x) < f(x_1) + \frac{f(x_2) - f(x_1)}{x_2 - x_1} \cdot (x - x_1) \; \forall x \in [a,b]$$
Una funzione $f:[a,b] -> \mathbb{R}$ si dice \textbf{concava} se $\forall x_1, x_2 \in [a,b]$ con $x_1 < x_2$ vale la seguente relazione
$$f(x) \geq f(x_1) + \frac{f(x_2) - f(x_1)}{x_2 - x_1} \cdot (x - x_1) \; \forall x \in [a,b]$$
\textbf{strettamente concava} se:
$$f(x) > f(x_1) + \frac{f(x_2) - f(x_1)}{x_2 - x_1} \cdot (x - x_1) \; \forall x \in [a,b]$$
Data una funzione continua $f:[a,b] \rightarrow \mathbb{R}$ possiamo affermare che
\begin{itemize}
    \item Essa è crescente (decrescente) in un punto $x \in [a,b]$ se la sua derivata prima è positiva (negativa) in $x$
    \item I \textbf{punti di stazionarietà} (estremanti) della funzione sono i punti in cui la derivata prima della funzione $f$ si annulla cambiando di segno,
    nello specifico si ha un punto di \textbf{massimo} in $x \in [a,b]$ quando $f'$ passa da un valore \textbf{positivo} a un valore \textbf{negativo}, mentre si ha un punto di
    \textbf{minimo} in $x \in [a,b]$ quando $f'$ passa da un valore $negativo$ a un valore $positivo$
    \item È detta \textbf{lineare} se la sua \textbf{derivata prima è una funzione costante}
\end{itemize}
\begin{center}
    \includegraphics[width = 0.50\textwidth]{Images/1.PNG}
\end{center}
Data una funzione continua $f:[a,b] \rightarrow \mathbb{R}$ e un punto $x_0 \in [a,b]$, si dice che $f$ ha un minimo o massimo locale (o relativo) nel punto 
$x_0$ quando esiste un intorno $l(x_0)$ nel quale risulta
\begin{itemize}
    \item $f(x) \geq f(x_0) \forall x \in l(x_0)$ allora $x_0$ è un \textbf{minimo locale}
    \item $f(x) \leq f(x_0) \forall x \in l(x_0)$ allora $x_0$ è un \textbf{massimo locale}
    \item $x_0$ è un \textbf{minimo locale relativo} se la funzione è decrescente immediatamente a sinistra di $x_0$ e crescente immediatamente a destra
    \item $x_0$ è un \textbf{massimo locale relativo} se la funzione è crescente immediatamente a sinistra di $x_0$ e decrescente immediatamente a destra 
\end{itemize}
Il punto minimo (massimo) locale in cui la funzione $f$ assume il valore minimo (massimo) viene detto \textbf{minimo} (\textbf{massimo}) \textbf{globale} o \textbf{assoluto}.
\subsection{Funzioni in due o più variabili}
Una funzione continua definita come $f: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ che associa ad ogni coppia di numeri reali $(x_1, x_2) \in \mathbb{R} \times \mathbb{R} = R^2$ uno
e un solo valore $y \in \mathbb{R}$ viene detta \textbf{funzioni in due variabili} $(x_1, x_2)$, che vengono dette \textbf{variabili indipendenti}, mentre la variabile $y$ viene riferita con il termine di
\textbf{variabile dipendente}.
Questo concetto è generalizzabile al caso in cui si considerino $n$ variabili indipendenti $(x_1, x_2, ..., x_n) \in \mathbb{R}^n$. In questo caso
si parla di funzione $f: \mathbb{R}^n \rightarrow \mathbb{R}$ in $n$ variabili indipendenti, funzione che descrive una "regola" per ottenere dall'insieme delle $n$ variabili indipendenti $(x_1, x_2, ..., x_n)$ un singolo
valore reale di $y$. \newline
Una funzione in $n$ variabili $f: \mathbb{R}^n \rightarrow \mathbb{R}$ viene detta \textbf{funzione lineare} nelle variabili $(x_1, x_2, ..., x_n)$ se è nella forma:
$$f(x_1, x_2, ..., x_n) = a_0 + a_1 \cdot x_1 + a_2 \cdot x_2 + ... + a_n \cdot x_n$$
dove $a_0, a_1, ..., a_n$ sono parametri che assumono valore reale. \newline
Una funzione in $n$ variabili $f: \mathbb{R}^n \rightarrow \mathbb{R}$ viene detta \textbf{funzione quadratica} nelle variabili $(x_1, x_2, ..., x_n)$ se è nella forma:
$$f(x_1, x_2, ..., x_n) = a_0 + \sum_{k=1}^n b_k \cdot x_k + \sum_{i = 1}^n \sum_{j \neq i,1}^n h_{ij} \cdot x_i \cdot x_j + \sum_{k=1}^n h_{kk} \cdot x_k^2$$
\begin{center}
    \includegraphics[width = 0.35\textwidth]{Images/2.PNG}
\end{center}
Le \textbf{curve di livello} di una funzione $f: \mathbb{R}^n \rightarrow \mathbb{R}$ sono ottenute disegnando i punti
$(x_1, x_2, ..., x_n)$ in cui la funzione ha valore constante $k$, vale a dire tutti i punti $(x_1, x_2, ..., x_n) \in \mathbb{R}^n$ per i quali vale la seguente uguaglianza
$$f(x_1, x_2, ..., x_n) = k$$
\begin{center}
    \includegraphics[width = 1\textwidth]{Images/3.PNG}
\end{center}
Dal punto di vista geometrico, le linee di livello sono le \textbf{proiezioni ortogonali} sul piano $Oxy$ delle curve ottenute
intersecando il piano $z=k$ e il grafico della funzione $z = f(x_1, x_2, ..., x_n)$
\begin{center}
    \includegraphics[width = 0.50\textwidth]{Images/4.PNG}
\end{center}
Data la funzione in 2 variabili $f: \mathbb{R}^2 \rightarrow \mathbb{R}$:
\begin{itemize}
    \item Si dice \textbf{derivata parziale rispetto a $x_1$} la seguente funzione:
    $$\frac{\partial f(x_1, x_2)}{\partial x_1} = f_{x_1} = f'_{x_1}$$
    Essa rappresenta il tasso con cui varia la funzione $f(x_1, x_2)$ al variare della variabile $x_1$, quando sia fissato e mantenuto costante
    il valore della variabile $x_2$.
    \item Si dice \textbf{derivata parziale rispetto a $x_2$} la seguente funzione:
    $$\frac{\partial f(x_1, x_2)}{\partial x_2} = f_{x_2} = f'_{x_2}$$
    Essa rappresenta il tasso con cui varia la funzione $f(x_1, x_2)$ al variare della variabile $x_2$, quando sia fissato e mantenuto costante
    il valore della variabile $x_1$
    \item Si dice \textbf{gradiente} il vettore i cui coefficienti sono le derivate parziali della funzione $f(x_1, x_2)$ rispetto alle variabili $x_1$ e $x_2$, esso è denotato nel seguente modo:
    $$\nabla f(x_1, x_2) = \begin{pmatrix}
        \frac{\partial f(x_1, x_2)}{\partial x_1} \\
        \frac{\partial f(x_1, x_2)}{\partial x_2}
    \end{pmatrix} = \begin{pmatrix}
        f'_{x_1} \\
        f'_{x_2}
    \end{pmatrix}$$
\end{itemize}
Data la funzione in 2 variabili $f: \mathbb{R}^2 \rightarrow \mathbb{R}, f(x_1, x_2)$:
\begin{itemize}
    \item Si dice \textbf{derivata parziale seconda rispetto a $x_1$} e $x_1$ la seguente funzione:
    $$\frac{\partial}{\partial x_1} \frac{\partial f(x_1, x_2)}{\partial x_1} = f_{x_1, x_1} = f'_{x_1, x_1}$$
    \item Si dice \textbf{derivata parziale seconda rispetto a $x_1$} e $x_2$ la seguente funzione:
    $$\frac{\partial}{\partial x_1} \frac{\partial f(x_1, x_2)}{\partial x_2} = f_{x_1, x_2} = f'_{x_1, x_2}$$
    \item Si dice \textbf{derivata parziale seconda rispetto a $x_2$} e $x_1$ la seguente funzione:
    $$\frac{\partial}{\partial x_2} \frac{\partial f(x_1, x_2)}{\partial x_1} = f_{x_2, x_1} = f'_{x_2, x_1}$$
    \item Si dice \textbf{derivata parziale seconda rispetto a $x_2$} e $x_2$ la seguente funzione:
    $$\frac{\partial}{\partial x_2} \frac{\partial f(x_1, x_2)}{\partial x_2} = f_{x_2, x_2} = f'_{x_2, x_2}$$
\end{itemize}
In particolare:
$$\frac{\partial}{\partial x_1} \frac{\partial f(x_1, x_2)}{\partial x_2} = f_{x_1, x_2} = f'_{x_1, x_2} = \frac{\partial}{\partial x_2} \frac{\partial f(x_1, x_2)}{\partial x_1} = f_{x_2, x_1} = f'_{x_2, x_1}$$
Data la funzione in 2 variabili $f: \mathbb{R}^2 \rightarrow \mathbb{R}, f(x_1, x_2)$, si dice
\textbf{matrice Hessiana} la matrice quadrata delle derivate parziali:
$$H = \begin{pmatrix}
    f_{x_1, x_1} & f_{x_1, x_2} \\
    f_{x_2, x_1} & f_{x_2, x_2}
\end{pmatrix}$$
\textbf{Condizione necessaria del primo ordine}: Data la funzione in 2 variabili $f: \mathbb{R}^2 \rightarrow \mathbb{R}, f(x_1, x_2)$, un punto
$(x_1, x_2)$ può essere un punto critico (minimo, massimo o sella) solo se il suo gradiente nel punto $(x_1, x_2)$ è nullo:
$$\nabla f(x_1, x_2) = \begin{pmatrix}
    0 \\
    0
\end{pmatrix}$$
Non ne conosciamo però la natura! (Minimo? Massimo? Sella?) \newpage
\textbf{Condizioni sufficienti del secondo ordine}:
Supponiamo che $(x_1, x_2)$ sia un punto critico di $f(x_1, x_2)$. Calcoliamo il determinante della matrice Hessiana:
$$det(H) = f_{x_1,x_1} (x_1, x_2) \cdot f_{x_2 x_2}(x_1, x_2) - (f_{x_1, x_2}(x_1, x_2))^2$$
Abbiamo i seguenti casi:
\begin{itemize}
    \item $det(H) > 0$:
    \begin{itemize}
        \item $f_{x_1, x_1} > 0 \Rightarrow (x_1, x_2)$ è un minimo relativo di $f(x_1, x_2)$
        \item $f_{x_1, x_1} < 0 \Rightarrow (x_1, x_2)$ è un massimo relativo di $f(x_1, x_2)$
    \end{itemize}
    \item $det(H) < 0 \Rightarrow (x_1, x_2)$ è un punto di sella di $f(x_1, x_2)$
\end{itemize}
Data la funzione in 2 variabili $f: \mathbb{R}^2 \rightarrow \mathbb{R}, f(x_1, x_2)$, se la sua matrice Hessiana $H$ è tale per cui
$f_{x_1, x_1} > 0$ e $det(H) > 0$ allora la funzione è \textbf{convessa}. Se la funzione è  convessa, allora ogni punto di minimo e di massimo sono
\textbf{globali} poiché ammette solamente un punto dove il gradiente si annulla
\section{Modelli nella Ricerca Operativa}
Data una funzione
$$f: \mathbb{R}^n \rightarrow \mathbb{R}$$
la chiamiamo \textbf{funzione obbiettivo}.
Un \textbf{problema di ottimizzazione} è formulabile come segue:
\begin{equation*}
    \begin{array}{rrclcl}
    \displaystyle \textrm{opt} & f(x)\\
    \textrm{s.a.} & x \in X & X \subseteq \mathbb{R}^n
    \end{array}
\end{equation*}
$X$ è detta \textbf{regione ammissibile}, cioè l'insieme delle soluzioni $x$ ammissibili dal problema. Inoltre, $\textrm{opt} \in \{\textrm{min}, \textrm{max}\}$. \newline
Se $\textrm{opt} = \textrm{min}$, allora abbiamo un \textbf{problema di minimizzazione}, altrimenti un \textbf{problema di massimizzazione}. \newline
Le variabili che indicano i vincoli ai quali è soggetto il problema sono dette \textbf{variabili decisionali} e identificano una soluzione del problema. \newline
Quindi, un problema di ottimizzazione consiste nel determinare, se esistono, uno o più punti di minimo/massimo $\textbf{x}^*$, assegnazione di valori alle variabili decisionali $\textbf{x}$, della funzione obbiettivo $f$ tra i punti
\textbf{x} che appartengono alla regione ammissibile $X$.
\begin{center}
    \includegraphics[width = 1\textwidth]{Images/5.PNG}
\end{center}
In particolare, se alcune zone di $\mathbb{R}^n$ non sono ammissibili, si dice che non sono \textbf{eleggibili}. \newline
Quando parliamo di ottimizzazione di una funzione obbiettivo possiamo avere diversi tipi di ottimizzazione: \newline 
\textbf{Ottimizzazione NON vincolata}: la ricerca del/i punto/i di ottimo della funzione obbiettivo viene condotta su tutto lo spazio di definizione
(quindi $X = \mathbb{R}^n)$ della/e variabile/i di decisione \newline
\textbf{Ottimizzazione vincolata}: la ricerca del/i punto/i di ottimo della funzione obbiettivo viene condotta su un sottoinsieme proprio dello spazio di definizione
(cioè $X \subset \mathbb{R}^n)$ della/e variabile/i di decisione
\textbf{Ottimizzazione intera}: le variabili di decisione assumono solo valori interi (quindi $X = \mathbb{Z}^n)$ \newline
\textbf{Ottimizzazione binaria}: Le variabili assumono solo valore 0 e 1 (quindi $X \in \{0,1\}^n$) \newline
\textbf{Ottimizzazione mista}: Alcune variabili assumono valori interi mentre altre variabili assumono solo valori binari. \newline
Se non specificato altrimenti, si deve intendere che \textbf{le variabili decisionali assumono valori reali}.
\subsection{Programmazione matematica}
Quando l'insieme $X$ delle soluzioni ammissibili di un problema di ottimizzazione viene espresso attraverso un sistema di equazione e disequazione, esso prende il nome di problema di
\textbf{programmazione matematica} (PM).
In questo caso un \textbf{vincolo} è un espressione del tipo:
$$g_i(x) \begin{Bmatrix}
    \geq \\
    = \\
    \leq
\end{Bmatrix} 0$$
Con $g_i: X \rightarrow \mathbb{R}$ funzione generica che lega tra loro le variabili decisionali.
In generale, possiamo avere uno o più vincoli. \newline
La \textbf{regione ammissibile} è quindi definita dall'insieme dei vincoli del problema, cioè:
$$X = \left \{x \in \mathbb{R}^n \; con \; g_i(x) \begin{Bmatrix} \leq \\ = \\ \geq \end{Bmatrix}, i = 1,...,m \right \}$$
Osserviamo, quindi, che abbiamo $m$ vincoli ed $n$ variabili. Inoltre
\begin{itemize}
    \item Se $x \in X$ allora $x$ è soluzione \textbf{ammissibile}
    \item Se $x \not\in X$ allora $x$ \textbf{non è una soluzione ammissibile} (soluzione inammissibile)
\end{itemize}
In un problema di ottimizzazione, abbiamo le seguenti possibilità riguardo la regione ammissibile:
\begin{itemize}
    \item \textbf{Problema non ammissibile}: $X = \emptyset$ (regione ammissibile vuota, nessuna soluzione ammissibile, problema mal posto)
    \item \textbf{Problema illimitato}, cioè:
    \begin{itemize}
        \item $\forall c \in \mathbb{R}, \exists x_c \in X | f(x_c) \leq c$ se $\textrm{opt} = \textrm{min}$ (illimitato inferiormente)
        \item $\forall c \in \mathbb{R}, \exists x_c \in X | f(x_c) \geq c$ se $\textrm{opt} = \textrm{max}$ (illimitato superiormente)
    \end{itemize}
    \item \textbf{Problema con soluzione ottima unica}
    \item \textbf{Problema con più di una soluzione ottima} (anche \textbf{infinite}): tutte le soluzione ottime hanno egual valore della funzione obbiettivo
\end{itemize}
\begin{center}
    \includegraphics[width = 0.90\textwidth]{Images/6.PNG}
\end{center}
\subsection{Ottimi globali e ottimi locali}
La risoluzione di un problema di programmazione matematica consiste nel trovare una soluzione ammissibile che sia un \textbf{ottimo globale},
vale a dire un vettore $\textbf{x}^* \in X$ tale che:
\begin{itemize}
    \item $f(\textbf{x}^*) \leq f(x) \forall x \in X$ se $\textrm{opt} = \textrm{min}$
    \item $f(\textbf{x}^*) \geq f(x) \forall x \in X$ se $\textrm{opt} = \textrm{max}$
\end{itemize}
\begin{Osservazione}
    Un problema di ottimizzazione può avere:
    \begin{itemize}
        \item Più di un ottimo locale
        \item Più di un ottimo globale
    \end{itemize}
\end{Osservazione}
\begin{Osservazione}
    Un punto di ottimo globale è anche di ottimo locale
\end{Osservazione}
\begin{Osservazione}
    Nel caso di una funzione obbiettivo \textbf{convessa}, vi è un unico ottimo globale
\end{Osservazione}
Anche qui abbiamo diversi casi possibili:
\begin{itemize}
    \item \textbf{Programmazione lineare}: in questo caso ci troviamo davanti ad un problema con questa formulazione:
    $$\textrm{opt} \; f(x) = \textbf{c}^T \textbf{x} \; (lineare)$$
    La regione ammissibile è quindi formulabile in questo modo:
    $$X = \left \{x \in \mathbb{R}^n \bigg | g_i(x) \begin{Bmatrix} \leq \\ = \\ \geq \end{Bmatrix}, i = 1,...,m \right \}$$
    con $g_i(x) = \textbf{a}_j^T\textbf{x} - b_i$ vincoli \textbf{lineari}
    \item \textbf{Programmazione Lineare Intera}: in questo caso ci troviamo davanti ad un problema con questa formulazione:
    $$\textrm{opt} \; f(x) = \textbf{c}^T \textbf{x} \; (lineare)$$
    La regione ammissibile è quindi formulabile in questo modo:
    $$X = \left \{x \in \mathbb{Z}^n \bigg | g_i(x) \begin{Bmatrix} \leq \\ = \\ \geq \end{Bmatrix}, i = 1,...,m \right \}$$
    con $g_i(x) = \textbf{a}_j^T\textbf{x} - b_i$ vincoli \textbf{lineari}
    \item \textbf{Programmazione non lineare}: in questo caso ci troviamo davanti ad un problema con questa formulazione:
    $$\textrm{opt} \; f(x) \; (lineare \; o \; non \; lineare)$$
    La regione ammissibile è quindi formulabile in questo modo:
    $$X = \left \{x \in \mathbb{R}^n \bigg | g_i(x) \begin{Bmatrix} \leq \\ = \\ \geq \end{Bmatrix}, i = 1,...,m \right \}$$
    con $g_i(\textbf{x})$ vincoli \textbf{lineari} o \textbf{non lineari}. È importante notare come, in questo caso, almeno un vincolo o
    la funzione obbiettivo sono NON lineari
\end{itemize}
\section{Programmazione lineare}
La programmazione lineare (PL) è quella branca della ricerca operativa che si occupa di studiare algoritmi di risoluzione per problemi di ottimizzazione lineari.
Un problema di programmazione lineare è strutturato come segue:
$$\underset{\textbf{x} \in X}{\textrm{opt}}Z = \sum_{j=1}^{n} c_j \cdot x_j \; (Funzione \; obbiettivo \; Z \; con \; n, \; numero \; di \; variabili \; decisionali)$$
$$\sum_{j=1}^n a_{ij} \cdot x_j \leq b_i, i = 1,...,m \; (Vincoli: \; regione \; ammissibile \; X \; con \; m, numero \; di \; vincoli)$$
Con: \newline
$x_j$ \textbf{variabili decisionali} \newline
$\left. \begin{matrix}
        c_j \; \textbf{coefficienti di costo} \\
        a_{ij}\; \textbf{termini noti sinistri} \\
        b_i \; \textbf{termini noti destri} \\
\end{matrix}\right\}$ Parametri \newline
Un problema di programmazione lineare si poggia sulle seguenti \textbf{assunzioni implicite}:
\begin{itemize}
    \item \textbf{Proporzionalità}: il contributo di ogni variabile decisionale, al valore della funzione obbiettivo, è proporzionale rispetto al valore assunto dalla variabile stessa
    \item \textbf{Additività}: ogni funzione è la somma dei contributi delle variabili decisionali
    \item \textbf{Continuità}: qualunque valore delle variabili decisionali in $\mathbb{R}^n$ è accettabile
    \item \textbf{Certezza}: il valore assegnato ad ogni parametro è assunto essere noto o costante 
\end{itemize}
Vediamole nel dettaglio:
\subsection{Assunzione di Proporzionalità}
Il contributo di ogni attività al valore della \textbf{funzione obbiettivo} $Z$ è proporzionale al \textbf{livello dell'attività} $x_j$ secondo:
$$Z = \sum_{j=1}^n c_j \cdot x_j$$
Analogamente, il contributo di ogni attività al \textbf{vincolo "i"} è proporzionale al \textbf{livello di attività} $x_j$ secondo
$$\sum_{j=1}^n a_{ij} \cdot x_j \leq b_i$$
Vediamo un esempio:
\begin{center}
    \includegraphics[width = 0.40\textwidth]{Images/7.PNG}
\end{center}
\begin{center}
    \includegraphics[width = 0.45\textwidth]{Images/8.PNG}
\end{center}
\subsection{Assunzione di additività}
In un problema di programmazione lineare, il valore assunto da ogni funzione, sia essa \textbf{funzione obbiettivo} o vincolo, è dato dalla somma dei contributi individuali delle rispettive attività.
Vediamo un esempio:
\begin{center}
    \includegraphics[width = 0.95\textwidth]{Images/9.PNG}
\end{center}
\subsection{Assunzione di continuità}
Le variabili decisionali in un problema di programmazione lineare (PL) sono libere di assumere qualsiasi valore, inclusi valori non interi che soddisfino i vincoli funzionali ed i vincoli di non negatività.
In altri termini le variabili decisionali sono continue.
In alcune condizioni può accadere che le variabili decisionali non possano che assumere valori interi; in questi casi si parla di problema di programmazione lineare intera o a numeri interi.
\subsection{Assunzione di certezza}
Il valore assegnato ad ogni parametro di un problema di programmazione lineare è assunto essere noto con certezza e costante.
\subsection{Soluzione grafica ad un problema di programmazione lineare}
Per risolvere i problemi di programmazione lineare, possiamo adottare una \textbf{procedura grafica}, determinando i valori delle variabili decisionali
$x_1, x_2$ che rispettano i vincoli, ed al tempo stesso rendono massimo il valore $Z$ della funzione obbiettivo.
La \textbf{soluzione grafica} si compone di:
\begin{itemize}
    \item Disegno della regione ammissibile
    \item Determinazione dell'ottimo
\end{itemize}
\subsubsection{Vincolo di uguaglianza}
I vincoli $g_i(\textbf{x})$ possono essere:
\begin{itemize}
    \item \textbf{Rette}: $g_i(x) = 0$
    \item \textbf{Semipiani}: $g_i(x) \leq 0$
\end{itemize}
Un vincolo del tipo $a_1 \cdot x_1 + a_2 \cdot x_2 = b$ è una \textbf{retta nel piano}.
La retta è perpendicolare al vettore $\nu = (a_1, a_2)$. Abbiamo quindi i seguenti casi:
\begin{center}
    \includegraphics[width = 1\textwidth]{Images/10.PNG}
\end{center}
Come rappresentiamo però un semipiano?
\begin{enumerate}
    \item Disegniamo la retta associata $(a_1 \cdot x_1 + a_2 \cdot x_2 \leq b)$
    \item Scegliamo un punto non appartenente a tale retta (torna comodo 0)
    \begin{itemize}
        \item Se il punto verifica la disuguaglianza allora scegliamo il semipiano che lo contiene
        \item Altrimenti scegliamo l'altro semipiano
    \end{itemize}
\end{enumerate}
\begin{center}
    \includegraphics[width = 0.30\textwidth]{Images/11.PNG}
\end{center}
\subsubsection{Vincoli funzionali di $\leq$}
In maniera generalizzata, possiamo pensare che un problema di programmazione lineare è formulato in questo modo:
\begin{equation*}
    \begin{array}{ll}
    \displaystyle \textrm{opt} & \; Z = c_1 \cdot x_1 + c_2 \cdot x_2 +...+ c_n \cdot x_n\\
    \textrm{s.a.} & a_{11} \cdot x_1 + a_{12} \cdot x_2 +...+ a_{1n} \cdot x_n \leq b_1 \\
    \phantom{} & a_{21} \cdot x_1 + a_{22} \cdot x_2 +...+ a_{2n} \cdot x_n \leq b_2 \\
    \phantom{} &... ... ... + ... ... ... + ... + ... ... ... \leq ... \\
    \phantom{} & a_{m1} \cdot x_1 + a_{m2} \cdot x_2 +...+ a_{mn} \cdot x_n \leq b_m \\
    \phantom{} &x_1 \geq 0, x_2 \geq 0, x_3 \geq 0 ..., x_m \geq 0
    \end{array}
\end{equation*}
Con
\begin{itemize}
    \item $Z$ funzione obbiettivo
    \item $\left. \begin{matrix*}[l]
    a_{11} \cdot x_1 + a_{12} \cdot x_2 +...+ a_{1n} \cdot x_n \leq b_1 \\
    a_{21} \cdot x_1 + a_{22} \cdot x_2 +...+ a_{2n} \cdot x_n \leq b_2 \\
    ... ... ... + ... ... ... + ... + ... ... ... \leq ... \\
    a_{m1} \cdot x_1 + a_{m2} \cdot x_2 +...+ a_{mn} \cdot x_n \leq b_m \\
    \end{matrix*}\right\}$ Vincoli funzionali
    \item $x_1 \geq 0, x_2 \geq 0, x_3 \geq 0 ..., x_m \geq 0$ vincoli di non negatività
\end{itemize}
In particolare quindi:
\begin{itemize}
    \item $Z =$ valore della misura di prestazione
    \item $x_j =$ livello dell'attività j
    \item $c_j =$ incremento del valore della misura di prestazione $Z$ corrispondente all'incremento di un'unità del valore dell'attività $x_j$
    \item $b_i =$ quantità di risorsa "$i$" allocabile alle attività $x_j, j = 1,..,n$
    \item $a_{ij} =$ quantità di risorsa "$i$" consumata da ogni unità di attività $x_j, j=1,...,n$
\end{itemize}
\subsubsection{Vincoli funzionali di $\geq$ e =}
Generalizzando al caso con \textbf{$n$ variabili decisionali} ed \textbf{$m$ vincoli}, otteniamo la seguente formulazione di un \textbf{problema di programmazione lineare}: \newline
\textbf{Funzione obbiettivo}: $\textrm{opt}\; Z$ \newline
\textbf{Vincoli}:
\begin{equation*}
    \begin{array}{ll}
        a_{11} \cdot x_1 + a_{12} \cdot x_2 + ... + a_{1n} \cdot x_n \leq b_1 \\
        a_{21} \cdot x_1 + a_{22} \cdot x_2 + ... + a_{2n} \cdot x_n \geq b_2 \\
        ... \\
        a_{n1} \cdot x_1 + a_{n2} \cdot x_2 + ... + a_{nm} \cdot x_n \leq b_n \\
    \end{array}
\end{equation*}
In questo caso quindi, siamo nel caso per il quale \textbf{$x_2$ non è vincolata} da nessun valore.
Inoltre, ogni vincolo di $\geq$ e $=$ può essere riscritto nella seguente forma:
\begin{itemize}
    \item $g_i(x) \geq b_i \rightarrow -g_i(x) \leq b_i$
    \item $g_i(x) = b_i \rightarrow \begin{cases}
        g_i(x) \geq b_i \\
        g_i(x) \leq b_i
    \end{cases}$
\end{itemize}
\subsubsection{Regione ammissibile}
La regione ammissibile $X$ è data dal soddisfacimento dei vari vincoli (Rette e semipiani):
$$X = \left \{\textbf{x} \in \mathbb{R}^n \middle | g_i(x) = \begin{Bmatrix}
    \geq \\
    = \\
    \leq
\end{Bmatrix}0, i=1,...,m \right \} \; con \; g_i(x) = \textbf{a}_i^T \textbf{x} - b_i$$ 
dove $\textbf{a} \in \mathbb{R}^n, \textbf{b} \in \mathbb{R}^m$ \newline
La regione ammissibile, da un punto di vista geometrico, corrisponde ad un \textbf{poliedro convesso in $\mathbb{R}^n$}.
La regione ammissibile può essere \textbf{limitata} (\textbf{politopo}) o illimitata
\begin{center}
    \includegraphics[width = 0.65\textwidth]{Images/12.PNG}
\end{center}
Si possono verificare quattro soluzioni:
\begin{itemize}
    \item Il problema di programmazione lineare \textbf{ammette una sola soluzione ottima} in un \textbf{vertice del poligono convesso}
    che delimita la regione ammissibile.
    \item Il problema di programmazione lineare \textbf{ammette infinite soluzioni ottime} in un \textbf{lato del poligono convesso} che delimita la regione ammissibile
    se la direzione di decrescita è perpendicolare ad un lato del poligono
    \item Il problema di programmazione lineare \textbf{ammette infinite soluzioni} perché la regione ammissibile è illimitata e la funzione obbiettivo è illimitata superiormente (se è di massimizzazione)
    o inferiormente (se di minimizzazione)
    \item Il problema di programmazione lineare \textbf{non ammette soluzione} perché la regione ammissibile è vuota
\end{itemize}
Per risolvere quindi un problema di programmazione lineare graficamente dobbiamo quindi:
\begin{enumerate}
    \item Disegnare la regione ammissibile
    \item Cercare di massimizzare (minimizzare) la soluzione, riportando ciò che troviamo sul grafico
\end{enumerate}
In particolare, per problemi in due variabili $x_1$ e $x_2$, il punto due può essere visto come segue:
\begin{center}
    \includegraphics[width = 0.85\textwidth]{Images/13.png}
\end{center}
Riportiamo la regione ammissibile nel piano 3D e troviamo dei valori per $x_1, x_2$ che diano lo stesso valore
una volta sostituiti nella funzione obbiettivo (nell'immagine, abbiamo che i valori $x_1 = 0, x_2 = 5$ e $x_1 = 3, x_2 = 0$)
\begin{center}
    \includegraphics[width = 0.95\textwidth]{Images/14.png}
\end{center}
Continuando così, usiamo il piano creato dalla funzione obbiettivo nello spazio 3D per capire se stiamo uscendo dalla regione ammissibile o meno:
\begin{center}
    \includegraphics[width = 0.90\textwidth]{Images/15.png}
\end{center}
\begin{center}
    \includegraphics[width = 0.90\textwidth]{Images/16.png}
\end{center}
\subsection{Minimum cost flow problem}
Supponiamo di trovarci in questa situazione:
l'azienda \textbf{distribution unlimited} produrrà un nuovo prodotto in \textbf{due fabbriche differenti},
successivamente i prodotti verranno inviati a \textbf{due magazzini}, ogni fabbrica potrà spedire i propri prodotti ai due magazzini.
La rete distributiva è mostrata di seguito:
\begin{center}
    \includegraphics[width = 0.55\textwidth]{Images/17.png}
\end{center}
Il problema consiste nel determinare quante unità di prodotto spedire dalle due fabbriche \textbf{$F1$} e \textbf{$F2$} ai due magazzini
\textbf{$W1$} e \textbf{$W2$}, utilizzando anche il centro di distribuzione \textbf{$DC$}, con l'obbiettivo di \textbf{minimizzare i costi di spedizione}. \newline
Questo tipi di problema prendono il nome di \textbf{Minimum cost flow problems}. Risolviamo l'esempio come segue: \newline
Sette corsie di spedizione richiedono sette variabili decisionali:
$$x_{F1 \rightarrow F2}, x_{F1 \rightarrow W1}, x_{F1 \rightarrow DC}$$
$$x_{F2  \rightarrow DC}$$
$$x_{DC \rightarrow W2}$$
$$x_{W1 \rightarrow W2}$$
$$x_{W2 \rightarrow W1}$$
Troviamo i seguenti vincoli:
\begin{itemize}
    \item \textbf{Vincoli di non negatività}:
    \begin{equation*}
        \begin{array}{ll}
            x_{F1 \rightarrow F2}, x_{F1 \rightarrow W1}, x_{F1 \rightarrow DC} \geq 0 \\
            x_{F2  \rightarrow DC} \geq 0 \\
            x_{DC \rightarrow W2} \geq 0 \\
            x_{W1 \rightarrow W2} \geq 0  \\
            x_{W2 \rightarrow W1} \geq 0
        \end{array}
    \end{equation*}
    \item \textbf{Vincoli di capacità massima}
    \begin{equation*}
        \begin{array}{ll}
            x_{F1 \rightarrow F2} \leq 10 \\
            x_{DC \rightarrow W2} \leq 80
        \end{array}
    \end{equation*}
    \item \textbf{Vincoli di conservazione del flusso}: $outflow - inflow = unita' necessarie$ \newline
    Questo vincolo in sostanza impone che \textbf{la somma dei flussi entranti in ogni nodo deve essere pari al flusso in uscita dal nodo medesimo}
\end{itemize}
IL problema che desideriamo risolvere consiste nel determinare \textbf{quante unità di prodotto} spedire dalle due fabbriche $F1$ e $F2$ ai due magazzini
$W1$ e $W2$, utilizzando anche il \textbf{centro di distribuzione DC}, con l'obbiettivo di minimizzare il costo di spedizione. Il \textbf{costo di spedizione per unità di prodotto}
è indicato per ogni arco di spedizione.
La funzione obbiettivo è:
$$\min Z = 2 \cdot x_{F1 \rightarrow F2} + 4 \cdot x_{F1 \rightarrow DC} + 9 \cdot x_{F1 \rightarrow W1} + 3 \cdot x_{F2 \rightarrow DC} + x_{DC \rightarrow W2} + 3 \cdot x_{W1 \rightarrow W2} + 2 \cdot x_{W2 \rightarrow W1}$$
Una volta calcolati i vincoli visti sopra, possiamo risolvere il problema di programmazione lineare.
\subsection{Metodo del simplesso}
Il metodo del simplesso è un algoritmo per la risoluzione dei problemi di programmazione lineare.
Nel caso medio, il tempo computazionale dell'algoritmo è \textbf{lineare rispetto al numero di variabili}.
Nel caso peggiore, invece, può risultare \textbf{esponenziale}.
Esso è una procedura algebrica, tuttavia i suoi concetti base hanno radici geometriche.
Consideriamo la seguente regione ammissibile:
\begin{center}
    \includegraphics[width = 0.55\textwidth]{Images/18.png}
\end{center}
I \textbf{vertici} si trovano all'intersezione di coppie di frontiere di vincoli.
Per ogni problema di programmazione lineare con $n$ variabili decisionali, due vertici (soluzioni vertici)
si dicono \textbf{adiacenti} se condividono $n-1$ frontiere di vincoli.
Due vertici adiacenti sono collegati da un segmento che giace sull'intersezione delle frontiere dei vincoli condivisi.
Questo segmento viene detto \textbf{spigolo} della regione ammissibile.
\textbf{Non tutti i vertici tuttavia sono soluzioni del problema di programmazione lineare}; lo sono solamente quelli che giacciono sulla regione ammissibile.
L'interesse per i vertici adiacenti sta nella seguente proprietà di cui godono: \newline
\textbf{Test di ottimalità}: Si consideri ogni problema di programmazione lineare tale da ammettere almeno una soluzione ottimale.
Se una soluzione vertice \textbf{non ammette} soluzioni vertice a lei adiacenti con valore della funzione obbiettivo $Z$ migliore, allora la soluzione in questione è \textbf{ottimale}.
\begin{center}
    \includegraphics[width = 0.60\textwidth]{Images/19.PNG}
\end{center}
Come già detto, il metodo del simplesso è un \textbf{algoritmo} e quindi è formato da una serie di passi:
\begin{enumerate}
    \item \textbf{Inizializzazione}: Scegliere una soluzione iniziale da cui partire; possiamo sceglierne una qualunque, quindi cerchiamo in modo che sia vantaggiosa e che non richieda molte computazioni per essere identificata
    \item \textbf{Test di ottimalità}: valutiamo lo spostamento nei vertici adiacenti alla soluzione iniziale:
    \begin{itemize}
        \item Se \textbf{esiste almeno un vertice adiacente con valore della funzione obbiettivo $Z$ migliore di quello del vertice iniziale} allora ci spostiamo in quel vertice, purché esso appartenga alla regione ammissibile; in caso ci siano più vertici
        con valore della funzione obbiettivo migliore rispetto a quello iniziale, \textbf{ci spostiamo in quello che ha il valore migliore}. Ripetiamo questo passo fino a quando non troviamo vertici adiacenti con valore della funzione obbiettivo migliore rispetto al vertice in cui ci troviamo
        \item Se \textbf{non esiste almeno un vertice adiacente con valore della funzione obbiettivo $Z$ migliore di quello del vertice iniziale} allora quel vertice \textbf{è la soluzione ottimale del problema di programmazione lineare} 
    \end{itemize}
\end{enumerate}
Il metodo del simplesso si basa su \textbf{sei concetti chiave}:
\begin{itemize}
    \item \textbf{Concetto chiave 1}: Il metodo del simplesso ispeziona solo soluzioni ammissibili corrispondenti a vertici.
    \textbf{Per ogni problema di PL che ametta almeno una soluzione ottimale, trovarne una, richiede di trovare solamente il vertice ammissibile cui compete il miglior valore della funzione obbiettivo}(La sola restrizione è che il problema possegga vertici ammissibili. Ciò è garantito dal fatto che la regione ammissibile sia limitata).
    Dato che il numero di soluzioni ammissibili è generalmente infinito, ridurre il numero di soluzioni da ispezionare ad un numero finito e piccolo è una semplificazione notevole.
    \item \textbf{Concetto chiave 2}: Il metodo del simplesso è un algoritmo iterativo con la seguente struttura:
    \begin{enumerate}
        \item \textbf{Inizializzazione}: scelta di una soluzione
        \item \textbf{Test di ottimalità}: la soluzione è ottimale?
        \begin{itemize}
            \item \textbf{NO}: torna ad 1) per trovare una soluzione migliore di quella corrente
            \item \textbf{SI}: termina l'algoritmo
        \end{itemize}
    \end{enumerate}
    \item \textbf{Concetto chiave 3}: Quando sia possibile, l'inizializzazione del metodo del simplesso seleziona l'origine (i valori di tutte le variabili di decisione vengono posti uguali a 0) come soluzione iniziale.
    \textbf{Se vi sono molte variabili decisionali, tali da rendere difficile usare il metodo grafico per scegliere la soluzione iniziale, scegliere l'origine evita di ricorrere a procedure algebriche pr determinare la soluzione iniziale del metodo del simplesso}(La soluzione nulla potrebbe però essere una soluzione non ammissibile. Se questo accade, vengono usate procedure specifiche per la scelta della soluzione iniziale)
    \item \textbf{Concetto chiave 4}: Dato un vertice, è più vantaggioso, in termini computazionali, acquisire informazioni sui vertici a lui adiacenti di quanto non sia per i vertici a lui non adiacenti. \textbf{Ad ogni iterazione, se l'algoritmo si sposta dal vertice corrente, verso un vertice con valore migliore della funzione obbiettivo, lo fa per muoversi in un vertice a lui adiacente. Nessuna altra soluzione viene considerata}.
    Pertanto, l'intero cammino, che partendo dalla soluzione iniziale raggiunge quella ottimale, attraversa spigoli della regione ammissibile
    \item \textbf{Concetto chiave 5}: A partire dal vertice corrente, il metodo del simplesso valuta i vertici ad esso adiacenti, ma non lo fa calcolando il valore della funzione obbiettivo per ognuno di essi.
    \textbf{Il metodo del simplesso valuta e compara i tassi di miglioramento della funzione obbiettivo $Z$ lungo la direzione degli spigoli che conducono dal vertice corrente ai vertici adiacenti}.
    Tra i vertici adiacenti con un tasso di miglioramento positivo per la funzione obbiettivo $Z$, il metodo del simplesso sceglie di muoversi lungo lo spigolo cui compete il massimo valore di incremento.
    \textbf{Il vertice selezionato diviene il nuovo vertice corrente}
    \item \textbf{Concetto chiave 6}: Il precedente concetto chiave descrive come il metodo del simplesso esamina gli spigoli che emanano dal vertice corrente.
    \textbf{L'ispezione di uno spigolo consente di identificare rapidamente il tasso di miglioramento di $Z$ che si otterrebbe muovendosi lungo di esso verso la soluzione adiacente all'altro estremo}.
    Un tasso di miglioramento positivo per $Z$ significa che il vertice adiacente è una soluzione migliore della soluzione corrente. Un tasso negativo implica che il vertice adiacente è una soluzione peggiore della soluzione corrente.
    \textbf{Il TEST DI OTTIMALITÀ consiste nel verificare se esiste uno spigolo con tasso positivo di miglioramento. Se tale condizione non è soddisfatta allora la soluzione corrente è ottimale}
\end{itemize}
\subsection{Procedura algebrica per il metodo del simplesso}
Fino ad ora abbiamo parlato dell'algoritmo del simplesso in termini geometrici.
Comunque, l'algoritmo del simplesso usualmente viene eseguito sun un calcolatore, il quale è in grado di interpretare solo istruzioni algebriche.
Pertanto, è necessario \textbf{tradurre la procedura geometrica in una procedura algebrica}.
La procedura algebrica si basa sulla \textbf{risoluzione di un sistema di equazioni lineari}.
Pertanto il primo passo da compiere per tradurre la procedura geometrica in procedura algebrica richiede di \textbf{tradurre i vincoli funzionali di disuguaglianza in vincoli funzionali di eguaglianza}.
I \textbf{vincoli di non negatività vengono mantenuti invariati} in quanto la loro \textbf{trattazione} viene effettuata in maniera \textbf{separata}.
Per convertire i vincoli di diseguaglianza in vincoli di uguaglianza, bisogna introdurre il concetto di \textbf{variabile slack}. \newline
Una \textbf{variabile slack} è una variabile che indica la \textbf{la quantità che manca al termine sinistro di una disuguaglianza affinche questa sia verificata con il segno di uguaglianza}.
Facciamo un esempio:
\begin{center}
    \includegraphics[width = 1\textwidth]{Images/20.PNG}
\end{center}
Introducendo una variabile slack per ognuno dei vincoli del problema di programmazione lineare in forma originale (\textbf{forma standard}) otteniamo una formulazione equivale detta:
\textbf{Modello in forma aumentata}:
\begin{center}
    \includegraphics[width = 0.70\textwidth]{Images/21.PNG}
\end{center}
Se la \textbf{variabile slack di un vincolo assume valore zero}, allora la soluzione corrispondente giace sulla frontiera del vincolo della forma originale. (Il vincolo corrispondente della forma originale è verificato come uguaglianza). \newline
Se la \textbf{variabile slack di un vincolo assume valore positivo}, la soluzione corrispondente appartiene al semipiano ammissibile individuato dalla frontiera del vincolo della forma originale, vale a dire la soluzione appartiene alla regione ammissibile (è interna alla regione ammissibile). \newline
Se la \textbf{variabile slack di un vincolo assume valore negativo}, la soluzione corrispondente appartiene al semipiano non ammissibile della frontiera del vincolo della forma originale, vale a dire la soluzione non appartiene alla regione ammissibile. \newline
Si definisce \textbf{SOLUZIONE AUMENTATA}, una soluzione del modello in forma originale (valori della variabili decisionali) che viene "aumentata" tramite i corrispondenti valori delle variabili slack \newline
Si definisce \textbf{SOLUZIONE DI BASE} un vertice del modello in forma aumentata. Una soluzione di base può essere \textbf{ammissibile} o \textbf{non ammissibile}.
Si dice \textbf{SOLUZIONE DI BASE AMMISSIBILE} una soluzione associata a un \textbf{vertice ammissibile}, che venga aumentata.
\subsubsection{Variabili di base e non}
I termini \textbf{soluzione di base} e \textbf{soluzione di base ammissibile} sono molto importanti ed è quindi necessario chiarirne le \textbf{proprietà algebriche}:
per esempio, due variabili poste a 0 $x_1 = 0$ e $x_4 = 0$ vengono dette \textbf{variabili non di base}. Prendiamo il modello in forma aumentata sopra come esempio.
La \textbf{soluzione del sistema lineare} per le 3 variabili restanti, dette \textbf{variabili di base} $x_2 = 6, x_3 = 4, x_5 = 6$ porta alla seguente \textbf{soluzione di base}:
$$x_1 = 0, x_2 = 6, x_3 = 4, x_4 = 0, x_5 = 6$$
Il modello in forma aumentata consiste di 5 variabili (2 decisionali e 3 slack) e di 3 equazioni.
Abbiamo a disposizione quindi $2 = (5-3)$ \textbf{gradi di libertà} per risolvere il sistema lineare (il metodo del simplesso pone a zero il valore di due variabili, scelte arbitrariamente, per risolvere il sistema lineare). \newline
Le \textbf{proprietà algebriche} delle \textbf{SOLUZIONI DI BASE} e delle \textbf{SOLUZIONI DI BASE AMMISSIBILI} sono descritte tramite le seguenti definizioni: \newline
Una \textbf{SOLUZIONE DI BASE} gode delle seguenti proprietà:
\begin{enumerate}
    \item Una variabile può essere una \textbf{VARIABILE DI BASE} o una \textbf{VARIABILE NON DI BASE}
    \item IL numero delle variabili di base eguaglia il numero dei vincoli funzionali (equazioni). Pertanto, il numero delle variabili non di base eguaglia il numero totale delle variabili meno il numero dei vincoli funzionali
    \item Le variabili non di base vengono poste a zero
    \item I valori delle variabili di base sono ottenuti come una risoluzione simultanea del sistema di equazioni lineari (vincoli funzionali in forma aumentata). L'insieme delle variabili di base viene spesso riferito con il termine di \textbf{BASE}
    \item Se le variabili di base soddisfano i vincoli di non negatività, la \textbf{SOLUZIONE DI BASE} è una \textbf{SOLUZIONE AMMISSIBILE DI BASE}
\end{enumerate}
Così come certe coppie di vertici ammissibili sono tra loro adiacenti, le corrispondenti coppie di \textbf{soluzioni di base ammissibili} sono tra loro adiacenti: \newline
Due \textbf{SOLUZIONI DI BASE AMMISSIBILI} sono adiacenti se sono caratterizzate dal condividere le stesse \textbf{VARIABILI NON DI BASE} eccetto una. Questo implica che tutte le loro \textbf{VARIABILI DI BASE} sono uguali eccetto una, anche se esse possono assumere valori differenti.
Conseguentemente, muoversi dalla \textbf{SOLUZIONE DI BASE AMMISSIBILE} corrente ad una soluzione ad essa adiacente implica che una \textbf{VARIABILE NON DI BASE} divenga una \textbf{VARIABILE DI BASE} e che una \textbf{VARIABILE DI BASE} divenga una \textbf{VARIABILE NON DI BASE} (il che richiede di aggiustare i valori delle variabili di base per garantire che il sistema lineare di equazioni sia ancora soddisfatto).
Sempre considerando la regione ammissibile sopra, possiamo quindi notare che:
\begin{center}
    \includegraphics[width = 0.60\textwidth]{Images/22.png}
\end{center}
Quando si considera il problema in forma aumentata, è vantaggioso \textbf{considerare e manipolare la funzione obbiettivo} insieme ai vincoli.
Pertanto, prima di passare ad impiegare il metodo del simplesso, il problema deve essere riscritto in una forma equivalente che risulti adeguata.
Tenendo sempre come esempio il modello in forma aumentata presente sopra, esso quindi diventa:
\begin{center}
    \includegraphics[width = 1\textwidth]{Images/23.png}
\end{center}
\subsubsection{Algoritmo del metodo del simplesso in forma algebrica}
Colleghiamo l'aspetto algebrico a quello geometrico per il metodo del simplesso.
L'\textbf{interpretazione geometrica} fa riferimento al \textbf{MODELLO IN FORMA STANDARD}. \newline
L'\textbf{interpretazione algebrica} fa riferimento al \textbf{MODELLO IN FORMA AUMENTATA}.
Presentiamo quindi l'algoritmo del metodo del simplesso in forma algebrica tenendo come esempio il modello presentato sopra:
\begin{enumerate}
    \item \textbf{INIZIALIZZAZIONE}: scegliere $x_1$ e $x_2$ come variabili non di base (perciò poste a valore nullo) per ottenere la soluzione iniziale di base ammissibile. Questo procedimento è giustificato dal \textbf{CONCETTO CHIAVE 3}
    \item \textbf{TEST DI OTTIMALITÀ}: $x_3, x_4$ e $x_5$ non compaiono nelle funzione obbiettivo $Z$, per cui sono i coefficienti delle variabili non di base, $x_1$ e $x_2$, a determinare il \textbf{TASSO DI CRESCITA} di $Z$.
    I \textbf{TASSI DI CRESCITA} sono rispettivamente 3 e 5, entrambi positivi, quindi la soluzione trovata sarà sicuramente \textbf{NON OTTIMALE}.
\end{enumerate}
Trattiamo inoltre come determinare la direzione di spostamento, assumendo di aver già trovato la soluzione di base iniziale:
\begin{enumerate}
    \item \textbf{DETERMINAZIONE DELLA DIREZIONE DI SPOSTAMENTO}: \newline aumentare il valore di una variabile non di base a partire da zero (modificano i valori delle variabili di base correnti per soddisfare le equazioni) equivale a spostarsi lungo uno degli spigoli che emanano dal corrente vertice ammissibile.
    La scelta di quale variabile non di base incrementare è effettuata in base al \textbf{CONCETTO CHIAVE 4} e al \textbf{CONCETTO CHIAVE 5}. Nel nostro esempio, aumenteremo $x_2$ poiché il suo tasso di miglioramento in $Z$ è 5, il quale è migliore del tasso di miglioramento in $Z$ dell'altra variabile non di base $x_1$.
    L'incremento del valore di questa variabile non di base, la fa divenire una variabile di base nella nuova soluzione di base; essa prende quindi il nome di \textbf{VARIABILE ENTRANTE}
    \item \textbf{DETERMINAZIONE DELL'INCREMENTO}: determina di quanto aumentare il valore della variabile entrante in base (nel nostro caso, $x_2$). Incrementare il valore di $x_2$ fa incrementare il valore della funzione obbiettivo $Z$, pertanto desideriamo aumentare il più possibile il valore di $x_2$ evitando però di abbandonare la regione ammissibile.
    Il soddisfacimento dei \textbf{vincoli} implica che se $x_2$ aumenta, mentre si mantiene il valore della variabile non di base $x_1$ uguale a zero, allora il valore di qualche altra variabile di base \textbf{deve variare}. L'ammissibilità richiede inoltre che \textbf{tutte le variabili siano non negative}. Le variabili non di base (inclusa quella entrante $x_2$) sono non negative, ma dobbiamo verificare quanto $x_2$ possa aumentare senza violare il vincolo di non negatività sulle variabili
    di base $x_3, x_4, x_5$. Per farlo, applichiamo il \textbf{TEST DEL RAPPORTO MINIMO}: prendiamo in considerazione tutti i vincoli a cui è soggetta la funzione obbiettivo e \textbf{ricaviamo da essi il massimo valore che possiamo assegnare a $x_2$ che non porti a violare i vincoli di non negatività sulle altre variabili}. Questo valore sarà quindi il più piccolo trovato:
    \begin{center}
        \includegraphics[width = 0.60\textwidth]{Images/24.png}
    \end{center}
    Questo passo dell'algoritmo va quindi a determinare quale variabile di base diminuisce a zero per prima quando si incrementa il valore della \textbf{VARIABILE ENTRANTE} in base.
    Diminuire il valore della variabile di base fino a raggiungere il valore zero trasforma questa variabile da variabile di base in variabile non di base nella nuova soluzione di base ammissibile.
    Pertanto, questa variabile è chiamata \textbf{VARIABILE USCENTE} dalla base all'iterazione corrente. 
    \item \textbf{DETERMINAZIONE DELLA NUOVA SOLUZIONE DI BASE}: Obbiettivo di questo passo è convertire il sistema lineare in una forma maggiormente vantaggiosa (forma adatta all'applicazione dell'\textbf{eliminazione Gaussiana}), al fine di applicare il test di ottimalità e (se necessario) di ottenere una nuova soluzione di base ammissibile.
    Nel nostro esempio, i valori di $x_3$ e $x_5$ nella nuova soluzione di base ammissibile vengono ottenuti come risultato di questo processo. Consideriamo di nuovo l'esempio
    Tenendo sempre come esempio il modello in forma aumentata presente sopra, esso quindi diventa:
    \begin{center}
        \includegraphics[width = 0.50\textwidth]{Images/25.png}
    \end{center}
    $x_2$ ha rimpiazzato $x_4$ come variabile di base nell'equazione (2). RIsolvere il sistema lineare rispetto a $Z, x_2, x_3, x_5$ richiede di applicare operazioni algebriche per \textbf{riprodurre il pattern $(0,0,1,0)$} di $x_4$ per $x_2$ (cioè fare in modo che il vettore colonna di $x_2$ diventi uguale al vettore colonna di $x_4$).
    Possiamo usare due tipi di operazioni algebriche:
    \begin{itemize}
        \item Moltiplicare (dividere) un'equazione per una costante non nulla
        \item Sommare (sottrarre) un multiplo di un'equazione per ottenere un'altra \newline equazione
    \end{itemize}
    \begin{center}
        \includegraphics[width = 1\linewidth]{Images/26.png}
    \end{center}
    \begin{center}
        \includegraphics[width = 1\linewidth]{Images/27.png}
    \end{center}
    Questo processo è detto \textbf{eliminazione di Gauss-Jordan}: ogni variabile di base viene eliminata da tutte le equazioni tranne una dove ha coefficiente 1
\end{enumerate}
Applichiamo questo procedimento fino a quando i coefficienti della funzione obbiettivo \textbf{sono tutti negativi}, cioè abbiamo solo tassi di miglioramenti negativi.
La soluzione trovata in quel punto è la \textbf{soluzione ottimale}.
\subsection{Procedura tabellare per il metodo del simplesso}
La forma algebrica è la migliore per comprendere la logica sottostante il metodo del simplesso.
Comunque, la forma algebrica non è la più adeguata per effettuare le computazioni che abbiamo mostrato in precedenza, molto meglio in questo caso utilizzare la \textbf{forma tabellare} che
registra solo l'informazione essenziale:
\begin{itemize}
    \item Coefficienti delle variabili
    \item Termini noti delle equazioni
    \item Variabili di base per ogni equazione
\end{itemize}
La \textbf{forma tabellare} evita di memorizzare i simboli delle variabili in ogni equazione, ma cosa ancor più importante è rappresentata dal fatto che, è possibile evidenziare i numeri che sono coinvolti nelle computazioni ed è possibile memorizzare in modo
compatto tali numeri. La tabella da utilizzare ha la seguente struttura:
\begin{center}
    \includegraphics[width = 1\textwidth]{Images/28.png}
\end{center}
La tabella sopra è stata riempita \textbf{basandosi sull'esempio presentato fino ad ora}. Vediamo, sempre utilizzando questo esempio, tutti i passi del metodo del simplesso svolti con la forma tabellare:
\begin{itemize}
    \item \textbf{INIZIALIZZAZIONE}: Assumendo che il problema sia in forma standard, si procede come segue:
    \begin{itemize}
        \item Aggiungere le variabili slack
        \item Selezionare le variabili di decisione da porre a 0 (non di base)
        \item Selezionare le variabili slack come variabili di base
    \end{itemize}
    \begin{center}
        \vspace{-0.3cm}
        \includegraphics[width = 1\linewidth]{Images/29.png}
        \vspace{-1cm}
    \end{center}
    La parte della tabella che comprende le colonne dei valori di $Z, x_1, ..., x_5$ e dei termini noti viene detto \textbf{tableau}.
    Ricordiamo che, in questo caso, le variabili non di base sono $x_1$ e $x_2$ e che la soluzione di base ammissibile è $(0, 0, 4, 12, 8)$
    \item \textbf{TEST DI OTTIMALITÀ}: La \textbf{soluzione di base ammissibile corrente} è ottimale solo se tutti i \textbf{coefficienti della riga (0)} sono non negativi.
    Se questo è il caso, ci si arresta, altrimenti si effettua un'iterazione per ottenere una nuova soluzione di base ammissibile, il che implica che una variabile non di base venga trasformata in una variabile di base (passo 1)
    e viceversa (passo 2), per poi ottenere una nuova soluzione (passo 3).
    Vediamo i vari passi da effettuare in caso ci troviamo nella situazione in cui la soluzione non è ottimale:
    \begin{itemize}
        \item \textbf{ITERAZIONE: PASSO 1}: Identificare la \textbf{variabile entrante in base} (selezionandola tra quelle non di base) come quella cui corrisponde il \textbf{minimo coefficiente negativo nell'equazione (0)}.
        La colonna corrispondente viene denominata \textbf{colonna pivot}.
        \begin{center}
            \includegraphics[width = 1\linewidth]{Images/30.png}
            \vspace{-0.7cm}
        \end{center}
        In questo caso, la variabile entrante sarà $x_2$.
        \item \textbf{ITERAZIONE: PASSO 2}: Determinate la \textbf{variabile di base uscente} tramite il \textbf{test del rapporto minimo}:
        \begin{enumerate}
            \item Selezionare i coefficienti strettamente positivi della colonna pivot
            \item Dividere i termini noti per questi coefficienti (riga omologa)
            \item Selezionare la riga cui corrisponde il più piccolo rapporto calcolato nel punto 2
            \item La variabile di quella riga è la variabile di base uscente, rimpiazzarla con la variabile entrante
        \end{enumerate}
        \begin{center}
            \includegraphics[width = 1\linewidth]{Images/31.png}
        \end{center}
        \item \textbf{ITERAZIONE: PASSO 3}: Determinare la nuova \textbf{soluzione di base} applicando operazioni che consentano l'uso dell'\textbf{eliminazione Gaussiana}.
        \begin{enumerate}
            \item Dividere la \textbf{riga pivot} per il \textbf{numero pivot}, ottenendo un nuovo \textbf{numero pivot} ed una nuova \textbf{riga pivot}
            \item Ad ogni altra riga (inclusa la riga 0) che abbia coefficiente negativo nella colonna pivot, sommare a questa riga il prodotto del valore assoluto del coefficiente per la nuova riga pivot
            \item Ad ogni altra riga (inclusa la riga 0) che abbia coefficiente positivo nella colonna pivot, sottrarre a questa riga il prodotto del valore assoluto del coefficiente per la nuova riga pivot
        \end{enumerate}
        \begin{center}
            \includegraphics[width = 0.87\textwidth]{Images/32.png}
        \end{center}
        \item \textbf{TEST DI OTTIMALITÀ}: Controlliamo che la nuova riga 0 abbia tutti coefficienti non negativi; se è questo il caso ci fermiamo, altrimenti effettuiamo una nuova iterazione dell'algoritmo
    \end{itemize}
\end{itemize}
\newpage
\subsection{Tie Breaking}
Fino ad ora non abbiamo detto nulla in relazioni a situazioni nelle quali le regole adottate non portino a decisioni univoche, a causa di casi multipli o di ambiguità di vario genere.
I casi possibili, che tratteremo, sono i seguenti:
\begin{itemize}
    \item Alternative multiple per la variabile entrante in base
    \item Alternative multiple per la variabile uscente dalla base
    \item Mancanza di uan variabile uscente (funzione obbiettivo illimitata)
    \item Molteplici soluzioni ottimali
\end{itemize}
\subsubsection{Variabile entrante}
Il \textbf{passo 1} di ogni iterazione del \textbf{metodo del simplesso} sceglie come variabile entrante in base, la variabile non dib ase cui corrisponde il minimo valore (negativo) del coefficiente nell'equazione (0).
Supponiamo che due o più variabili non di base abbiano lo stesso valore minimo (negativo) per i rispettivi coefficienti dell'equazione (0). La scelta di quale variabile debba entrare in base può essere effettuata arbitrariamente.
La soluzione ottimale verrò comunque ottenuta, indipendentemente dalla variabile scelta come variabile entrante in base, e non sono disponibili metodi per prevedere quale scelta condurrà più rapidamente alla soluzione ottimale.
\subsubsection{Variabile uscente}
Supponiamo che al \textbf{passo 2 dell'algoritmo del simplesso} due o più variabili di base competano per uscire di base. \textbf{Fa differenza quale variabile viene scelta per uscire di base?}
\textbf{Assolutamente si}, è la fa in termini molto critici in base alla seguente catena di eventi:
\begin{itemize}
    \item Quando il valore della variabile di entrante viene aumentato, le variabili di base che sono selezionabili come variabili uscenti dalla base, raggiungono contemporaneamente il valore zero. Pertanto, le nuove variabili non selezionate come variabili uscenti avranno comunque
    un valore nullo nella nuova soluzione di base (una variabile di base che assume valore nullo è detta \textbf{variabile degenere})
    \item Se una \textbf{variabile degenere} mantiene il proprio valore nullo sino ad un'iterazione successiva, dove viene selezionata come variabile uscente, la corrispondente variabile entrante in base deve rimanere nulla (dato che non può essere aumentata senza far assumere un valore negativo alla variabile uscente).
    Pertanto il valore della funzione obbiettivo $Z$ resta invariato
    \item Se il valore della funzione obbiettivo $Z$ resta costante invece di aumentare ad ogni iterazione, il \textbf{metodo del simplesso potrebbe entrare in loop}, ripetendo la medesima sequenza di soluzioni senza raggiungere la soluzione ottimale. Sono stati prodotto esempi sintetici che mostrano come il metodo del simplesso venga
    \textbf{intrappolato in un loop perpetuo}. Tuttavia questo accade raramente e sono state sviluppate strategie apposite per controllare questa \textbf{situazione patologica}
\end{itemize}
\subsubsection{Nesuna variabile uscente}
Il \textbf{passo 2} di ogni iterazione del metodo del simplesso può portare ad un'ulteriore esito che non abbiano ancora discusso, vale a dire che nessuna variabile di base si qualifichi come variabile uscente di base (la stessa cosa non può accadere al \textbf{passo 1}, in quanto il test di ottimalità arresterebbe l'algoritmo).
Tale situazione si verifica se il valore della variabile entrante può essere aumentato illimitatamente senza implicare che il valore di almeno una variabile di base divenga negativo.
Nella \textbf{forma tabellare} significa che ogni coefficiente della \textbf{colonna pivot} (esclusa la riga 0) assume valore non positivo.
Il tableau quindi si può interpretare in questo modo: i vincoli non impediscono che il valore della funzione obbiettivo $Z$ cresca illimitatamente, pertanto il metodo del simplesso si arresta segnalando che il \textbf{valore della funzione obbiettivo $Z$ è illimitato}.
\subsubsection{Molteplici soluzioni ottimali}
Precedentemente, quando abbiamo introdotto il concetto di \textbf{SOLUZIONE OTTIMALE}, abbiamo precisato che \textbf{un problema di programmazione lineare può avere più di una soluzione ottimale}.
Quindi:
\begin{itemize}
    \item Ogni problema di PL che ammetta soluziono ottimali multiple (con regione ammissibile limitata) ha almeno \textbf{due vertici ammissibili} che sono \textbf{ottimali}
    \item Ogni soluzione ottimale è \textbf{combinazione convessa} di questi vertici ammissibili ottimali, cioè ogni soluzione è \textbf{esprimibile tramite una combinazione lineare del tipo}:
    $$\lambda_1 \boldsymbol{v_1} + \lambda_2 \boldsymbol{v_2}$$
    con $\boldsymbol{v_1}, \boldsymbol{v_2}$ i vertici ottimali e $\lambda_1 + \lambda_2 = 1$
    \item Di conseguenza, nella forma aumentata del problema di PL, \textbf{ogni soluzione ottimale risulta essere una combinazione convessa delle soluzioni di base ammissibili ottimali}.
\end{itemize}
Il metodo del simplesso si arresta automaticamente non appena individua una soluzione di base ottimale.
Comunque, in molte applicazioni di PL, sussistono fattori non tangibili, non inclusi nel modello di PL, sfruttabili per scegliere adeguatamente tra le soluzioni ottimali alternative. In questo caso, le soluzioni alternative devono essere comunque individuate.
Come menzionato, questo richiede di trovare tutte le altre soluzioni di base ottimali, e quindi ogni soluzione ottimale sarà calcolabile come combinazione convessa di tali soluzioni ottimali di base.
Una volta che il metodo del simplesso ha individuato una soluzione di base ottimale, è possibile identificare se ne esistono altre, ed in caso affermativo, trovarle come segue: se un problema ha più di una soluzione di base ottimale,
almeno \textbf{una delle variabili non di base ha un coefficiente nullo nella riga (0)}, il che implica che incrementare il valore di una qualsiasi di tali variabili \textbf{non cambia il valore della funzione obbiettivo $Z$}. Pertanto, queste altre
soluzioni di base ottimali sono identificabili (se necessario) effettuando ulteriori iterazioni del metodo del simplesso, scegliendo ogni volta una variabile non di base con coefficiente nullo come variabile entrante.
\subsection{Forme alternative}
Sino ad ora abbiamo presentato come applicare il metodo del simplesso assumendo che il problema di PL da risolvere fosse in \textbf{FORMA STANDARD}, cioè:
\begin{itemize}
    \item Problema di massimizzazione
    \item Vincoli di $\leq$
    \item Variabili di decisione non negative
\end{itemize}
Ora mostriamo come ricondurre un problema di PL in forma generica nel corrispondente problema in forma standard.
Tale trasformazione avverrà nella fase di inizializzazione, consentendo di applicare il metodo del simplesso esattamente come mostrato per la risoluzione della forma standard.
Consideriamo il seguente problema di programmazione lineare:
\begin{equation*}
    \begin{array}{ll}
        \displaystyle \textrm{min} & \; Z = -2 \cdot x_1 + 3 \cdot x_2\\
        \textrm{s.a.} & x_1 + x_2 = 7\\
        \phantom{} & x_1 - 2 \cdot x_2 \leq 4\\
        \phantom{} & x_1 \geq 0 \\
    \end{array}
\end{equation*}
Notiamo che:
\begin{itemize}
    \item È un problema di minimizzazione e non di massimizzazione
    \item C'è un vincolo di $=$
    \item Manca un vincolo di non negatività per $x_2$
\end{itemize}
Come facciamo a trasformarlo in un problema di PL in forma standard?
Per trasformare il problema di minimizzazione in un problema di massimizzazione, applichiamo la seguente formula:
$$\textrm{min} \; Z = -\textrm(max)\;(-Z)$$
Moltiplichiamo quindi la funzione obbiettivo per $-1$, ottenendo:
\begin{equation*}
    \begin{array}{ll}
        \displaystyle \textrm{max} & \; Z = 2 \cdot x_1 - 3 \cdot x_2\\
        \textrm{s.a.} & x_1 + x_2 = 7\\
        \phantom{} & x_1 - 2 \cdot x_2 \leq 4\\
        \phantom{} & x_1 \geq 0 \\
    \end{array}
\end{equation*}
Come facciamo a risolvere l'assenza del vincolo di \textbf{non negatività} per $x_2$?
Rimpiazziamo $x_2$ con \textbf{due variabili non negative} $x_2'$ e $x_2''$ con $x_2 = x_2' - x_2''$, ottenendo:
\begin{equation*}
    \begin{array}{ll}
        \displaystyle \textrm{max} & \; Z = 2 \cdot x_1 - 3 \cdot x_2' + 3\cdot x_2''\\
        \textrm{s.a.} & x_1 + x_2' - x_2''= 7\\
        \phantom{} & x_1 - 2 \cdot x_2' + 2 \cdot x_2'' \leq 4\\
        \phantom{} & x_1, x_2', x_2'' \geq 0 \\
    \end{array}
\end{equation*}
Come facciamo a trasformare un vincolo di $=$? Questo tipo di vincoli \textbf{possono essere sostituiti da due vincoli, uno di $\geq$ ed uno di $\leq$}.
Quindi otteniamo:
\begin{equation*}
    \begin{array}{ll}
        \displaystyle \textrm{max} & \; Z = 2 \cdot x_1 - 3 \cdot x_2' + 3\cdot x_2''\\
        \textrm{s.a.} & x_1 + x_2' - x_2'' \leq 7\\
        \phantom{} & x_1 + x_2' - x_2 '' \geq 7 \\
        \phantom{} & x_1 - 2 \cdot x_2' + 2 \cdot x_2'' \leq 4\\
        \phantom{} & x_1, x_2', x_2'' \geq 0 \\
    \end{array}
\end{equation*}
Moltiplichiamo per $-1$ il vincolo di $\geq$ e trasformiamolo in un vincolo di $\leq$:
\begin{equation*}
    \begin{array}{ll}
        \displaystyle \textrm{max} & \; Z = 2 \cdot x_1 - 3 \cdot x_2' + 3\cdot x_2''\\
        \textrm{s.a.} & x_1 + x_2' - x_2''\leq 7\\
        \phantom{} & -x_1 - x_2' + x_2 '' \leq 7 \\
        \phantom{} & x_1 - 2 \cdot x_2' + 2\cdot x_2'' \leq 4\\
        \phantom{} & x_1, x_2', x_2'' \geq 0 \\
    \end{array}
\end{equation*}
Infine, rinominiamo le variabili per \textbf{consistenza notazionale}, ottenendo:
\begin{equation*}
    \begin{array}{ll}
        \displaystyle \textrm{max} & \; Z = 2 \cdot x_1 - 3 \cdot x_2 + 3\cdot x_3\\
        \textrm{s.a.} & x_1 + x_2 - x_3 \leq 7\\
        \phantom{} & -x_1 - x_2 + x_3 \leq 7 \\
        \phantom{} & x_1 - 2 \cdot x_2 + 2 \cdot x_3 \leq 4\\
        \phantom{} & x_1, x_2, x_3 \geq 0 \\
    \end{array}
\end{equation*}
È sempre possibile convertire un problema di PL in forma standard. Riportiamo la seguente tabelle riassuntiva:
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/33.png}
\end{center}
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/34.png}
\end{center}

\end{document}
