\documentclass[12pt]{article}

\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage[utf8]{inputenc}
\usepackage{changepage}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{biblatex}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
\SetKwProg{Proc}{Procedure}{:}{}


\lstset{
  language=Python,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  tabsize=4,
  basicstyle=\ttfamily,
  columns=fullflexible,
  keepspaces,
}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\newenvironment{para}{\begin{adjustwidth}{13mm}{}}{\end{adjustwidth}}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\newcommand{\tabitem}{\llap{\textbullet}}
\newcommand{\Hsquare}{%
\text{\fboxsep=-.2pt\fbox{\rule{0pt}{1ex}\rule{1ex}{0pt}}}%
}

\newtheorem{Definizione}{Definizione}[subsection]
\newtheorem{Lemma}{Lemma}[subsection]
\newtheorem{Teorema/Definizione}{Teorema/Definizione}[subsection]
\newtheorem{Corollario}{Corollario}[subsection]
\newtheorem{Teorema}{Teorema}[subsection]
\newtheorem{Proposizione}{Proposizione}[subsection]
\newtheorem{Notazione}{Notazione}[subsection]
\newtheorem{Commento}{Commento}[subsection]
\newtheorem{Dimostrazione}{Dimostrazione}[subsection]
\newtheorem{Osservazione}{Osservazione}[subsection]
\newtheorem{Nota}{Nota}[subsection]

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\title{Introduzione all'intelligenza artificiale}
\author{spitfire}
\date{A.A. 2024-2025}
\begin{document}
\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{Images/Logo scienze bicocca.png}
\end{figure}

\vspace{10cm}
\date{A.A. 2024-2025}


\maketitle

\newpage

\tableofcontents
\newpage

\section{Introduzione}
Che cos'è l'intelligenza artificiale? Prima di tutto, dovremmo definire che cos'è \textbf{l'intelligenza}.
Negli anni sono state date molte definizioni:
\begin{itemize}
    \item "L'intelligenza è una capacità mentale molto \textbf{generale} che, tra le altre cose, coinvolge la capacità di \textbf{ragionare, pianificare, risolvere problemi, pensare in maniera astratta, comprende idee complesse, apprendere velocemente e imparare dall'esperienza}".
     da "Mainstream science on intelligence: An editorial with 52 signatories, history,
        and bibliography, Intelligence 24(1):13–23, 1997"
    \item "L'intelligenza è la \textbf{capacità di adattarsi efficacemente all'ambiente}, o cambiando se stessi o cambiando l'ambiente oppure trovandone uno nuovo... l'intelligenza non è un singolo processo mentale, ma piuttosto una combinazioni molti processi mentali indirizzati verso un adattamento efficace all'ambiente" da Encyclopedia Britannica, 2006.
    \item ... 
\end{itemize}
Quindi il problema di fondo è quello di \textbf{definire l'intelligenza}. Le relazioni tra l'informatica e le scienze cognitive non sono quindi
sporadici e sono particolarmente significativi. Uno schema proposto da \textbf{Russel e Norvig e il seguente}:
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/1.PNG}
\end{center}
essi propongono uno schema che "astrae" i tipi di intelligenza, ponendoli su due dimensioni:
\begin{itemize}
    \item Sull'asse delle ordinate troviamo il contrasto tra \textbf{l'imitare l'essere umano} (cioè imitare il suo modo di agire) e il \textbf{pensare come un umano} (quindi il "risolvere problemi")
    \item Sull'asse delle ascisse troviamo il \textbf{contrasto tra il pensare e l'agire}
\end{itemize}
\subsection{Storia dell'intelligenza artificiale}
Il termine "intelligenza artificiale" fu coniato nell'agosto del 1955 da \textbf{John McCarty, Marvin Minsky, Allan Newell e Herbert Simon}, i quali proposero al Dartmouth College di Hanover (New Hampshire) di organizzare il "Dartmouth College Summer Research Project on Artificial Intelligence"; cioè uno spazio
dove accogliere persone che volessero discutere del tema dell'intelligenza artificiale. Essi descrissero l'iniziativa nel seguente modo:
\begin{center}
    "Lo studio dovrà procedere sulla base della congettura che ogni aspetto dell'apprendimento o ogni altra caratteristica dell'intelligenza può essere sia, in linea di principio, descrivibile in maniera talmente precisa che una macchina può essere costruita per simularla"
\end{center} 
In realtà, la storia dell'intelligenza artificiale risale persino ad Alan Turing (1912-1954), il quale definì il cosiddetto \textbf{Test di Turing}: l'idea di questo test è che ci sia un essere umano $C$ separato fisicamente da due interlocutori, uno anch'esso umano, chiamato $B$, e l'altro una macchina, chiamata $A$, programmata in qualche modo.
$C$ non può interloquire direttamente con $A$ e $B$, tuttavia può scambiare messaggi con essi tramite un qualche sistema di comunicazione (foglietti di carta, una chat ecc...). Quando $C$, interagendo in maniera dialogica con i due interlocutori, non riesce più di una certa percentuale di volte a capire chi dei due è la macchina allora $A$ mostra un
comportamento intelligente (cioè "agisce come un essere umano", "agisce razionalmente").
\begin{center}
    \includegraphics[width = 0.50\linewidth]{Images/2.PNG}
\end{center}
Tuttavia, nel 1966, fu prodotto un programma chiamato \textbf{ELIZA}, il quale imitava uno \textbf{psicoterapeuta}, che \textbf{superò il test di Turing nonostante fosse basato su regole di pattern matching con espressioni regolari}. Il programma portava l'utente ad avere una
\textbf{conversazione plausibile}, cioè davano all'utente l'illusione di star parlando con un \textbf{essere umano}. Per questo, il test di Turing ha solo un \textbf{interesse storico} e risulta poco interessante.
Non è quindi un caso che, nel gruppo di studio citato sopra, una delle cose di cui ci si è occupati non fosse dedicata subito all'apprendimento ma al \textbf{ragionamento}.
\subsection{Approcci simbolici}
La disciplina che si occupa delle forme corrette di ragionamento è la \textbf{logica}.
La logica è quindi lo studio sistematico delle \textbf{forme di inferenza valida}, cioè forme di elaborazione e rappresentazione della conoscenza
che garantiscono che da informazioni vere si derivino informazioni vere.
La \textbf{logica computazionale} è l'uso della logica per effettuare ragionamenti riguardo alla computazione (es. dimostrazione della correttezza di un programma).
Alcuni sforzi iniziali nel settore dell'intelligenza artificiale erano legati alla realizzazione di \textbf{dimostratori automatici di teoremi}. \newline 
Un'\textbf{inferenza} è un ragionamento che stabilisce delle relazioni tra \textbf{premesse} e delle \textbf{conclusioni}. I \textbf{modelli computazionali} di inferenza si interessano quindi di:
\begin{itemize}
    \item Quali informazioni possono trarre dato un insieme di premesse?
    \item Perché la conclusione è corretta?
\end{itemize}
L'inferenza riguarda quindi il trarre delle conclusioni quando \textbf{le premesse sono vere}.
Dire che un'inferenza è corretta, tuttavia, \textbf{non dice nulla sul valore di verità delle conclusioni}, quindi un ragionamento può essere corretto anche se \textbf{le premesse sono false}.
Le varie forme di inferenza sono:
\begin{itemize}
    \item \textbf{Deduzione}:"Se le premesse sono vere, allora le conseguenze devono essere anch'esse vere". Questa forma di ragionamento parte da delle premesse \textbf{generali} e trae delle conclusioni \textbf{particolari}
    \item \textbf{Inferenze di "senso comune"}: Esse non sono sempre "valide", ma sono \textbf{utili nella pratica}; sono modelli per spiegare \textbf{quando un inferenza è giustificata} e calcolarla di conseguenza. Esempi di questo tipo di inferenze sono:
    \begin{itemize}
        \item \textbf{Induzione}: Questa forma di ragionamento parte da delle \textbf{osservazioni particolari} e arriva a delle \textbf{conclusioni generali}.
        \item \textbf{Abduzione}: Sillogismo in cui la premessa maggiore è certa e la premessa minore è probabile, per cui anche la conclusione risulta solo probabile.
    \end{itemize}
\end{itemize}
Dove si applica quindi l'IA di tipo simbolico?
\begin{itemize}
    \item Problemi che possono essere espressi in termini di \textbf{vincoli} che devono essere soddisfatti
    \item Situazioni in cui si devono studiare delle \textbf{sequenze di azioni} per portare un certo stato dell'ambiente circostante ad un determinato obbiettivo
\end{itemize}
In passato, si svilupparono applicazioni il cui obbiettivo era \textbf{risolvere problemi specifici e delimitati}; questa applicazioni presero il nome di \textbf{sistemi esperti}.
Degli esempi notevoli sono:
\begin{itemize}
    \item \textbf{Dendral}(Anni '60): Automatizzò il processo di decisione e l'approccio alla risoluzione dei problemi dei chimici organici
    \item \textbf{Mycin}(Anni '70): Supportò  l'identificazione dei batteri che causavano infezioni gravi e la raccomandazione di antibiotici 
\end{itemize}
\begin{center}
    \includegraphics[width = 0.80\linewidth]{Images/3.PNG}
\end{center}
Tuttavia, presto divennero evidenti i \textbf{limiti} dei sistemi esperti:
\begin{itemize}
    \item È necessario trovare un \textbf{esperto disposto a codificare la sua conoscenza all'interno della base di conoscenza}
    \item I sistemi esperti \textbf{non scalano bene con grandi quantità di informazioni}: l'aggiornamento del sistema esperto rispetto ai \textbf{nuovi approcci alla risoluzione del problema per cui è stato costruito} è particolarmente complesso, sopratutto se il problema \textbf{non è ben delimitato}. La base di conoscenza del sistema rischierebbe quindi di \textbf{contenere troppi assiomi e regole}.
    \item Il costo di sviluppo di questi sistemi è \textbf{elevato}
\end{itemize}
\subsection{Approcci sub-simbolici}
I sistemi di IA \textbf{sub-simbolici} \textbf{non manipolano una rappresentazione simbolica} per trovare soluzioni a problemi, ma effettuano \textbf{calcoli secondo alcuni principi} che hanno dimostrato di essere in grado di portare alla risoluzione del problema.
Esempi notevoli sono:
\begin{itemize}
    \item Algoritmi genetici
    \item Reti Neurali
    \item "Intelligenza dello sciame" (Swarm Intelligence)
\end{itemize}
Tuttavia, l'argomento più importante correntemente è quello del \textbf{Machine Learning}.
Le \textbf{reti neurali artificiali} (Artificial Neural Networks (ANN)) sono una simulazione astratta del nostro sistema nervoso, il quale
contiene una collezione di \textbf{neuroni} che comunicano tra loro tramite delle connessioni dette \textbf{assoni}.
\begin{center}
    \includegraphics[width = 0.60\linewidth]{Images/4.PNG}
\end{center}
Il modello delle ANN ha delle certe somiglianze con gli assoni e i dendriti nel nostro sistema nervoso.
Il primo modello di rete neurale artificiale fu proposto nel 1943 da \textbf{McCulloch} e \textbf{Pits} nei termini di un \textbf{modello computazione dell'attività neurale}.
Questo modello fu poi seguito da altri modelli proposti da \textbf{John Von Neumann, Marvin Minsky, Frank Rosenblatt} e molti altri.
Rosenblatt definì un modello "algebrico" del neurone, chiamato \textbf{percettrone}; esso è una pietra miliare della ricerca sulle reti neurali.
Il percettrone cerca di \textbf{simulare le operazioni svolte da un singolo neurone}; l'apprendimento quindi diventa un problema di \textbf{scegliere i pesi e le soglie corrette}.
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/5.PNG}
\end{center}
tendenzialmente, si arriva ad avere dei \textbf{percettroni multistrato}, dove ogni nodo è un singolo percettrone (introdotti per la prima volta da Minksy e S.Papert nel 1969)
\begin{center}
    \includegraphics[width = 0.40\linewidth]{Images/6.PNG}
\end{center}
come faccio però a determinare i pesi di tutta la rete? Si utilizza il meccanismo della \textbf{back-propagation}: l'idea è che l'input è una \textbf{rappresentazione del problema} e che vi sia un \textbf{output desiderato} che la rete deve offrire;
inizialmente la rete neurale ha \textbf{dei pesi causali}, che verranno corretti \textbf{retropropagando} l'output desiderato sulla rete.
Questo è un approccio all'apprendimento che si dice "\textbf{supervisionato}".
\begin{center}
    \includegraphics[width = 0.75\linewidth]{Images/7.PNG}
\end{center}
Oltre all'apprendimento supervisionato, esistono molte altre tecniche di addestramento:
\begin{center}
    \includegraphics[width = 0.95\linewidth]{Images/8.PNG}
\end{center}
\newpage
\subsection{Agenti intelligenti, architetture e ambienti}
Un \textbf{agente} è tutto ciò che può essere vista come "percepente il suo ambiente" attraverso dei \textbf{sensori} e che può \textbf{agire su tale ambiente} attraverso degli \textbf{attuatori}.
Come agenti possono essere quindi classificati
\begin{itemize}
    \item Gli \textbf{esseri umani}, definendo come "sensori" gli occhi, le orecchie ecc... e come attuatori la bocca, le gambe, le braccia ecc...
    \item Gli \textbf{agenti robotici}, i quali hanno telecamere e sensori ad infrarossi come sensori e vari motori come attuatori
    \item Nulla vieta che \textbf{un agente possa essere anche solamente un software}
\end{itemize} 
Un agente è quindi rappresentabile nel seguente modo:
\begin{center}
    \includegraphics[width = 0.45\linewidth]{Images/9.PNG}
\end{center}
la forma più generale di agente è una \textbf{funzione} che mappa \textbf{l'insieme potenza di tutte le percezioni istantanee $\mathcal{P}^*$} ad una azione dell'insieme \textbf{di tutte le azioni eseguibili dall'agente} $\mathcal{A}$, cioè:
$$f: \mathcal{P}^* \rightarrow \mathcal{A}$$
quindi, l'agente può mappare una \textbf{sequenza arbitrariamente lunga di percezioni istantanee} (insieme potenza di $\mathcal{P}$) \textbf{ad una singola azione contenuta nell'insieme $\mathcal{A}$}.
Un agente è quindi \textbf{la sua architettura (la sua struttura profonda) più il suo programma (specifico dell'agente)}.
Gli agenti possono essere classificati, in base alla loro architettura interna, nelle seguenti classi:
\begin{itemize}
    \item Agenti con riflessi semplici
    \item Agenti con riflessi basati su un modello
    \item Agenti basati su un obbiettivo e su un modello
    \item Agenti basati su un'utilità e un modello.
\end{itemize}
\subsubsection{Agenti con riflessi semplici}
Un agente con riflessi semplici è un agente in cui vi è una comunicazione con l'ambiente (tramite i sensori e gli attuatori dell'agente).
Al suo interno, i sensori vanno a realizzare una "vista" che rappresenta \textbf{lo stato attuale dell'ambiente circostante}.
L'agente deve quindi \textbf{scegliere quale azione intraprendere} e, in questo caso, per farlo ha solamente a disposizione delle regole del tipo
\textbf{condizione-azione}.
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/10.PNG}
\end{center}
questo tipo di agente è \textbf{privo di stato}. Con architetture estremamente semplici, magari con più agenti, si riescono
quindi ad ottenere dei comportamenti, se non intelligenti, perlomeno utili.
\subsubsection{Agenti con riflessi basati su modello}
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/11.PNG}
\end{center}
La differenza più significativa di questi agenti rispetto agli agenti con riflessi semplici è la \textbf{conoscenza del proprio stato interno}.
Questo tipo di agente può quindi \textbf{riflettere sul proprio stato} e quindi effettuare azioni basate su di esso.
Anche ignorando che l'ambiente ha una struttura, questo tipo di agente deve quindi avere un \textbf{modello matematico dell'evoluzione del mondo rispetto alle azioni fatte} che gli permetta
di decidere quale azioni intraprendere. Lo stato \textbf{non è una memoria che mantiene lo stato del mondo}, ma è solo un'informazione sullo stato dell'agente.
L'agente, inoltre, deve \textbf{poter percepire il proprio stato come parte dell'ambiente} (o come una "percezione esterna" o proprio come uno stato interno all'agente e che esso aggiorna e conosce).
Questo tipo di agente quindi può eseguire \textbf{comportamenti più adattivi rispetto all'ambiente}.
\subsubsection{Agenti basati su un obbiettivo e su un modello}
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/12.PNG}
\end{center}
Questo tipo di agente, tramite i sensori ed eventualmente l'informazione di stato, si fa un'idea dello stato in cui si trova.
Questo agente presenta una \textbf{funzione che, partendo dallo stato dell'agente, ritorna tutte le azioni ammissibili che l'agente può intraprendere}.
Lo stato attuale viene quindi messo come \textbf{radice di un albero} e generiamo, a partire da esso, un numero di figli pari al numero di azioni ammissibili.
Possiamo costruire questo albero perché sappiamo \textbf{in quale stato si andrà a finire eseguendo una determinata azione}.
Per ogni nodo di stato che viene generato in questo modo, potremmo generare \textbf{un ulteriore livello di approfondimento dell'albero}, cioè ogni nodo stato potrebbe diventare la radice di un ulteriore sotto-albero.
In linea di principio, potrei quindi costruire l'albero di \textbf{tutti gli stati raggiungibili possibili tramite ogni combinazione possibile di azioni}.
Ovviamente, il fattore di ramificazione di questo albero è \textbf{estremamente elevato}. L'agente deve quindi esplorare questo albero per capire se esistono delle configurazioni nelle quali \textbf{l'obbiettivo sia verificato}.
Vi è quindi una \textbf{costruzione dello spazio degli stati} e una \textbf{ricerca nello spazio degli stati}. \newline
Per capire quali azioni intraprende, l'agente deve quindi risolvere un \textbf{problema di ricerca}, il quale consiste in:
\begin{itemize}
    \item Uno \textbf{spazio degli stati}
    \item Una \textbf{funzione di successione}, che dato un certo stato e l'azione che si vuole intraprendere (la quale potrebbe avere un certo \textbf{costo}), in quale stato si vada a finire. Questa funzione mi dice anche \textbf{quali sono le azioni ammissibili in un determinato stato}
    \item Uno \textbf{stato iniziale}
    \item Una \textbf{funzione "goal test"} che ci dica \textbf{se l'obbiettivo è stato raggiunto o meno}
\end{itemize}
Una \textbf{soluzione} ad un problema di ricerca è una \textbf{sequenza di azioni} (un piano) che trasformano lo stato iniziale nello stato obbiettivo.
Uno \textbf{spazio di ricerca} astrae l'ambiente per selezionare solo le informazioni utili per risolvere il problema.
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/13.PNG}
\end{center}
lo spazio degli stati, per problemi di ragionevole complessità, \textbf{tendono ad esplodere}, quindi non si possono applicare generalmente algoritmi \textbf{forza bruta} per trovare una configurazione che risolva il problema.
La costruzione dello spazio di ricerca avviene tramite \textbf{un albero di ricerca}, dove:
\begin{itemize}
    \item Lo stato iniziale è il nodo radice
    \item I figli corrispondono agli stati successivi data un'azione
    \item I nodi mostrano gli stati, ma \textbf{corrispondono ai piani per raggiungere quelli stati}
    \item Per la maggior parte dei problemi, non arriviamo mai a costruire l'intero albero (troppo grande)
\end{itemize}
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/14.PNG}
\end{center}
Invece di usare un albero per rappresentare il problema di ricerca, potremmo invece usare un \textbf{grafo dello spazio degli stati}: esso da una rappresentazione matematica del problema di ricerca nel seguente modo:
\begin{itemize}
    \item I nodi sono \textbf{le possibili configurazioni del mondo}
    \item Gli archi rappresentano \textbf{i risultati delle azioni}
    \item La funzione \textbf{"goal test"} viene rappresentata da un \textbf{insieme di nodi}
\end{itemize}
\begin{center}
    \includegraphics[width = 0.50\linewidth]{Images/15.PNG}
\end{center}
In un grafo dello spazio degli stati, \textbf{ogni stato occorre solamente una volta!} Tuttavia, raramente possiamo costruire l'intero grafo in memoria, poiché \textbf{esso cresce troppo velocemente}; tuttavia è un'idea utile.
Qual'è quindi la differenza sostanziale tra un grafo dello spazio degli stati e un albero di ricerca? Ogni \textbf{nodo} in un albero di ricerca è l'equivalente di un \textbf{intero cammino sul grafo dello spazio degli stati}.
Entrambi devono essere costruiti \textbf{"on demand"} (cioè li espandiamo solamente quando occorre) e devono esser espansi il meno possibile.
\subsubsection{Agenti basati su un'utilità e un modello}
\begin{center}
    \includegraphics[width = 0.80\linewidth]{Images/16.PNG}
\end{center}
La differenza di questo tipo di agente con il precedente è la \textbf{scomparsa della funzione "goal test"} e l'introduzione di una \textbf{funzione "Utility"}, la quale è una vera e propria funzione
che attribuisce ad un certo stato del mondo un idea di \textbf{quanto quello stato sia desiderabile dall'agente}. Perché? Ci sono diversi motivi:
\begin{itemize}
    \item Potrei non essere in grado di definire formalmente una funzione che descriva l'obbiettivo
    \item Dato lo stato dell'ambiente, posso definire una funzione che valuti i vari elementi di esso e mi permetta di effettuare valutazione sulla prossima aziona da eseguire (es. scacchi)
    \item Avere una funzione d'utilità mi permette di considerare obbiettivi contrastanti fra loro; quindi poter definire una funzione che valuti tutti i fattori in gioco e che possa portare alla \textbf{discriminazione di certi stati} (es. problema di logistica)
\end{itemize}
\subsubsection{Caratteristiche dell'ambiente}
Russel e Norvig definiscono una serie di caratteristiche per l'ambiente:
\begin{itemize}
    \item \textbf{Ambienti accessibili vs Inaccessibili}: Un \textbf{ambiente accessibile} è un ambiente dove l'agente può ottenere un informazione \textbf{completa, accurata e aggiornata} riguardo lo stato dell'ambiente.
    Molti ambiente moderatamente complessi (includendo, per esempio, il mondo fisico e internet) sono \textbf{inaccessibili}. Più un ambiente è accessibile, più è semplice costruire un agente che possa operare in esso.
    I problemi di accessibilità del mondo si presentano ogni qualvolta che ci accingiamo a risolvere problemi nel \textbf{mondo fisico}
    \item \textbf{Ambienti deterministici vs non-deterministici}: Un \textbf{ambiente deterministico} è un ambiente in cui ogni azione ha un singolo effetto garantito: non c'è \textbf{incertezza} riguardo allo stato in cui l'ambiente si troverà in seguito al compiersi di un azione. Il mondo fisico dovrebbe essere sempre considerato come sostanzialmente \textbf{non-deterministico}, salvo certi casi in cui si può pensare come deterministico.
    Gli ambienti non-deterministici presentano problemi maggiore per il progettista dell'agente.
    \item \textbf{Ambienti episodici vs sequenziali}: In un ambiente \textbf{episodico}, l'esperienza dell'agente può essere divisa in \textbf{passi atomici} dove l'agente percepisce uno stimolo ed effettua una singola azione. La scelta dell'azione da intraprendere \textbf{dipende solamente dall'episodio stesso}.
    Gli ambienti episodici sono più semplici dal punto di vista del progettista dell'agente, poiché l'agente può decidere quale azione intraprendere basandosi solo sull'episodio corrente, \textbf{non deve quindi ragionare riguardo all'interazione tra questo episodio e quelli futuri}.
    \item \textbf{Ambienti statici vs dinamici}: Per \textbf{ambiente statico} si intende un ambiente che \textbf{non cambia nel mentre che l'agente sta decidendo l'azione da compiere}; un cambiamento in un ambiente statico avviene quindi \textbf{solamente a causa di un'azione da parte dell'agente}.
    Per \textbf{ambiente dinamico} si intende un ambiente che \textbf{cambia mentre l'agente sta decidendo quale azione intraprendere} e che quindi ha al suo interno altri processi, oltre all'agente, che ne modificano lo stato e le cui azioni possono interferire con le azioni dell'agente (come nella teoria dei sistemi concorrenti); le trasformazioni di un ambiente dinamico quindi avvengono \textbf{al di fuori del controllo dell'agente}.
    Il mondo fisico è un ambiente altamente dinamico.
    \item \textbf{Ambienti discreti vs continui}: Un \textbf{ambiente discreto} è un ambiente che presenta un numero \textbf{fissato e finito} di azioni e di percezioni in esso. Gli \textbf{ambienti continui} hanno un certo livello di \textbf{discrepanza} con i sistemi computerizzati.
    Gli ambienti discreti potrebbero essere gestiti, in linea di principio, da una specie di \textbf{tabella di ricerca} (lookup table).
    Per semplificare la gestione di un ambiente continuo, si può \textbf{sovrapporre ad esso una sua discretizzazione} in modo da rendere più semplice la progettazione e l'implementazione degli agenti che devono operare al suo interno.
\end{itemize} 
\begin{center}
    \includegraphics[width = 0.80\linewidth]{Images/17.PNG}
\end{center}
\section{Approcci simbolici - Planning}
Come abbiamo visto, definire in maniera precisa cos'è l'intelligenza è un compito piuttosto complicato.
La ricerca di una definizione univoca ha portato a definizioni di intelligenza \textbf{più focalizzate sulle singole funzioni} e ha portato alla seguente distinzione in termini di performance:
\begin{center}
    \includegraphics[width = 0.80\linewidth]{Images/18.PNG}
\end{center}
abbiamo quindi dato una definizione di intelligenza basandoci su dei \textbf{compiti da svolgere}, i quali possono essere a volte dei \textbf{problemi}.
Ma quindi, cos'è effettivamente un problema? Possiamo dare la seguente definizione:
\begin{center}
    Vogliamo \textbf{realizzare una condizione desiderata}, che all'inizio non è soddisfatta. Per farlo, dobbiamo \textbf{scegliere} quali azioni intraprendere da un insieme di \textbf{possibili scelte (complesse)}.
\end{center}
Uno degli interessi di studio nel campo dell'intelligenza artificiale è anche quello di \textbf{comprendere come gli essere umani risolvono i problemi}.
Possiamo quindi partire dalla nozione di \textbf{risolutore di problemi generici}, di cui una versione possibile è la seguente:
\begin{center}
    \includegraphics[width = 0.60\linewidth]{Images/19.PNG}
\end{center}
quando questa soluzione può essere inefficiente oppure quando essa può fallire?
\begin{itemize}
    \item Una fonte di inefficienza può essere la \textbf{necessità di effettuare un test su tutte le condizioni generate}
    \item Il costo di una soluzione \textbf{potrebbe essere elevato sia dal punto di vista economico che umano}
    \item Generare le soluzioni ha un costo
    \item Questo sistema può fallire non solo \textbf{quando le possibili scelte sono tante} ma anche quando il generatore \textbf{non riesce a generare tutte le possibili soluzioni oppure esse sono infinite} 
\end{itemize}
nonostante le problematiche sopra, il fatto di \textbf{poter enumerare e valutare tutte le possibili soluzioni al problema} è già un modo di risolverlo quando il suo spazio delle soluzioni (e quindi delle scelte) è limitato.
Ci sono varie condizioni dove si possono applicare, in maniera efficiente, questo tipo di risolutori; alcuni esempi sono:
\begin{itemize}
    \item \textbf{Risolvere puzzle}: In questo caso, la condizione desiderata è \textbf{raggiungere la configurazione base del puzzle} (es. il cubo di rubik con tutte le facce di un certo colore) in una certa maniera (più velocemente? con meno azioni possibili? ecc...)
    La condizione iniziale \textbf{è una configurazione randomica del puzzle}. In questo caso, dobbiamo produrre la sequenza di tutte le azioni necessarie per arrivare alla condizione desiderata rispettando i limiti di tempo/mosse dati.
    Le \textbf{scelte} in questo caso sono \textbf{le sequenza di cambiamenti (complessi) nella configurazione del puzzle}, mentre le \textbf{azioni} sono invece ciò che ci permette di muoverci da una configurazione del puzzle all'altra per creare, concatenandole in sequenza, delle \textbf{scelte} che possano essere valutate rispetto al nostro obbiettivo.
    Ovviamente, per esempio, raggiungere l'obbiettivo con il minor numero di mosse possibili \textbf{non è detto che sia la maniera più veloce} e viceversa.
    \newpage
    \item \textbf{Pacman}:
    \begin{itemize}
        \item \textbf{Condizione desiderata}: mangiare la pillola più vicina? Mangiare tutte le pillole il più velocemente possibile?
        \item \textbf{Inizialmente}: Pacman si trova da qualche parte all'interno del labirinto
        \item \textbf{Scelte}: Strada 1, Strada 2, ..., Strada $N + 1$
        \item \textbf{Azioni}: su, giù, sinistra, destra
    \end{itemize}
    \item ...
\end{itemize}
ciò che hanno in comunque questi esempi è che entrambi presentano delle \textbf{condizioni}, cioè degli \textbf{stati}.
Partendo da una configurazione iniziale e compiendo un'azione, ci troviamo in una configurazione successiva, in cui possiamo compiere
un'altra azione ed arrivare ad un'altra configurazione. L'iterazione di questo processo ci permette di creare \textbf{sequenze di cambiamenti} (le scelte) e quindi di \textbf{generare ogni possibile configurazione del problema}.
Abbiamo quindi ottenuto il generatore $G$ del risolutore sopra.
La possibilità di testare se la configurazione soddisfa l'obbiettivo dipende da come definiamo la configurazione stessa.
Ciò che resta da fare è quindi trovare dei buoni modi per implementare i generatori di soluzioni $G$ e per testare le soluzioni che generano velocemente.
\subsection{Problemi di ricerca}
Come abbiamo visto, le soluzioni possono essere costruite come \textbf{sequenze di stati (o configurazioni e azioni che le cambiano)}.
I tipi di problemi che abbiamo visto sopra si dicono \textbf{problemi di ricerca}, nei quali abbiamo:
\begin{itemize}
    \item Uno \textbf{spazio degli stati} (insieme degli stati) $\mathcal{S}$
    \item Uno \textbf{stato iniziale} $s_0$
    \item Un \textbf{insieme delle azioni possibili in ogni stato} $\mathcal{A}(\mathcal{s})$
    \item Una \textbf{funzione di successione/transizione} $s' = next$
    \item Un \textbf{"goal test"} $G(s)$ che dice se abbiamo raggiunto l'obbiettivo
    \item Un \textbf{costo per ogni azione} $c(s, a, s')$ (opzionale)
    \item 
\end{itemize}
Una \textbf{soluzione ad un problema di ricerca} è una sequenza di azioni (un piano) che trasforma lo stato iniziale in uno \textbf{stato obbiettivo}, cioè in uno stato che soddisfi il \textbf{goal test}.
Una \textbf{soluzione ottimale} ha il \textbf{costo minore tra tutte le possibili soluzioni}.
Lo stato di un problema \textbf{può essere rappresentato in molti modi diversi} (es. pixel, spazio degli stati ecc...).
Per evitare di complicare troppo il problema di ricerca definendo uno spazio degli stati inefficiente, possiamo seguire le \textbf{condizioni di Markov sugli stati, sui successori e sul goal test}:
\begin{itemize}
    \item L'efficienza richiede un \textbf{insieme degli stati il più piccolo possibile}; tuttavia esso deve soddisfare le seguente condizioni:
    \begin{enumerate}
        \item Data una funzione di transizione, uno stato \textbf{contiene tutte le informazioni necessarie per arrivare all'obbiettivo tramite la scelta di azioni da intraprendere}
        \item Dato lo stato $s(t)$, il risultato $s(t+1) = next(s(t), a(t))$ della prossima azione $a(t)$ non dipende dagli stati precedenti $s(t')$ o dalle azioni precedenti
        \item Il goal test può essere applicato direttamente agli stati senza che sia richiesta dell'altra informazione aggiuntiva.
    \end{enumerate}
\end{itemize}
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/20.PNG}
\end{center}
La condizione in cui risolvere un problema di ricerca risulta più semplice è quando \textbf{il numero degli stati e delle azioni è finito} e \textbf{per ogni azione, c'è un solo possibile stato di destinazione che si raggiunge}.
Per risolvere un problema con la ricerca, ciò che possiamo fare è quindi generare \textbf{tutte le configurazioni a cui le azioni possibili in un certo stato $s$ portano} e poi scegliere quali
di esse espandere ulteriormente. La scelta di \textbf{quale configurazione espandere determina il comportamento dell'agente durante la risoluzione del problema} (esso, per esempio, potrebbe anche entrare in un loop infinito).
Facciamo un esempio: dato il seguente problema:
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/21.PNG}
\end{center}
allora un algoritmo che l'agente può seguire per risolvere il problema è il seguente:
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/22.PNG}
\end{center}
il modo in cui estraiamo una soluzione o ne aggiungiamo una nuova influenza l'efficienza dell'algoritmo.
Cambiando la struttura dati sottostante, viene inoltre cambiato \textbf{il comportamento dell'agente e le sue performance}.
Consideriamo ora un \textbf{agente basato su riflessi}; quando l'agente riceve un input $x$, allora esso effettua l'azione associata
a quell'input; quindi, se $f$ è la funzione che determina quale azione intraprendere, si può vedere l'agente in questo modo:
$$x \Rightarrow f \Rightarrow \textrm{singola azione } y \in \{-1, +1\}$$
Tuttavia, è piuttosto complesso costruire agenti di questo tipo per risolvere problemi di ricerca. Si preferisce quindi avere
agenti \textbf{basati sullo stato} che effettuano una simulazione interna e che siano in grado di \textbf{tenere traccia delle conseguenze delle proprie azioni} per arrivare alla soluzione:
$$x \Rightarrow f \Rightarrow \textrm{sequenza di azioni } (a_1, a_2, \dots)$$
tuttavia, abbiamo bisogno di questo tipo di agenti? Non potremmo risolvere questi problemi "senza pensare"?
\begin{itemize}
    \item Potremmo, in linea di principio, usare un agente basato su riflessi per risolvere un problema di ricerca? Potremmo creare una tabella di ricerca per determinare ogni nuovo stato a partire a seconda dell'azione intrapresa, ma solo sotto certe condizioni:
    \begin{itemize}
        \item L'informazione disponibile deve essere sempre sufficiente per poter effettuare una scelta
        \item Se non ci sono problemi di informazione, allora si potrebbe utilizzare una mappa degli stati per risolvere il problema di ricerca
    \end{itemize}
    \item Il problema principale a questo tipo di approccio è \textbf{il numero delle possibili configurazioni del problema}, visto che per ogni possibile configurazione è necessario avere una soluzione.
    \item Poiché le configurazione possono essere moltissime, allora la mappa rischia di essere enorme
\end{itemize}
Dobbiamo infine ricordare che quando si risolve un problema di ricerca, utilizziamo un \textbf{modello} che astrae le informazioni necessarie a risolverlo.
Per questo motivo, \textbf{i modelli, rispetto al mondo reale, risultano quasi sempre errati}, poiché tengono conto solamente delle informazioni rilevanti per risolvere il problema.
\subsection{Search Trees e algoritmi di ricerca}
Un modo molto semplice per codificare le soluzioni parziali ed effettuare delle scelte sono gli \textbf{alberi di ricerca} (search trees).
In questo tipo di struttura dati è:
\begin{itemize}
    \item Ogni nodo è uno \textbf{stato}
    \item La radice è lo \textbf{iniziale}
    \item I figli di ogni nodo corrispondono ai suoi \textbf{successori}
    \item Una \textbf{soluzione} è un \textbf{cammino} che dalla radice porta ad una foglia che soddisfa il "Goal Test"
\end{itemize}
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/23.PNG}
\end{center}
Un problema di questo approccio è che, a causa dell'elevato numero di stati, \textbf{non è quasi mai possibile espandere completamente l'albero di ricerca}.
Si rende quindi necessario lo sviluppo di approcci per ridurre i tempi computazionali.
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/24.PNG}
\end{center}
la differenza, quindi, tra i vari algoritmi di ricerca è il \textbf{modo in cui si sceglie di costruire ed esplorare l'albero}.
Un algoritmo generico per la \textbf{ricerca all'interno dell'albero è il seguente}:
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/25.PNG}
\end{center}
questa è un'implementazione basata sull'albero; la questione quindi diventa:
\begin{itemize}
    \item Quale nodo foglia espandere?
    \item Dobbiamo fare un controllo per stati ripetuti? (loop infiniti)
\end{itemize}
le proprietà di un algoritmo di ricerca generico sono le seguenti:
\begin{itemize}
    \item \textbf{Caratteristiche della ricerca}:
    \begin{itemize}
        \item \textbf{Completezza}: Garantisce di trovare una soluzione se esiste?
        \item \textbf{Ottimalità}: Garantisce di trovare il cammino di costo minimo?
    \end{itemize}
    \item Sia $b$ il \textbf{branching factor}, cioè il numero di azioni possibili per stato
    \item Sia $m$ (o $D$) la \textbf{massima profondità} a cui si può esplorare
    \item Le soluzioni possono essere a varie profondità
    \item Numero di nodi nell'intero albero? $$1 + b + b^2 + \dots + b^m = \mathcal{O}(b^m)$$
\end{itemize}
\begin{center}
    \includegraphics[width = 0.80\linewidth]{Images/29.PNG}
\end{center}
\subsubsection{Backtracking search}
Un primo algoritmo semplice che ci permette di trovare una soluzione nell'albero di ricerca
è il \textbf{Backtracking search}:
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/26.PNG}
\end{center}
questo algoritmo espande sempre il primo nodo che non è stato espanso, fino ad arrivare ad una certa profondità.
Il limite alla profondità dell'albero permette di evitare che ci si possa fermare a causa di loop infiniti.
In questo modo, l'algoritmo \textbf{ricostruisce l'intero albero dei percorsi di lunghezza massima}, permettendo di poter effettuare
diverse domande sul nostro spazio degli stati. (es. percorso di lunghezza minima, percorso di costo minimo ecc...).
Lo svantaggio di questo algoritmo è che \textbf{deve espandere l'intero albero per trovare una soluzione} e non è detto che l'ultima soluzione trovata dall'algoritmo
\textbf{sia migliore di una soluzione trovata in precedenza}; tuttavia, questo tipo di algoritmo è estremamente flessibile, quindi si possono avere
delle condizioni arbitrarie che \textbf{impediscono all'algoritmo di continuare ad espandere l'albero in una certa direzione}.
Se abbiamo $b$ azioni possibili per stato, e un massima profondità $D$ di ricerca, allora i costi dell'algoritmo sono i seguenti:
\begin{itemize}
    \item \textbf{Memoria}: $\mathcal{O}(bD)$ (piccolo)
    \item \textbf{Tempo}: $\mathcal{O}(b^D)$ (enorme)
\end{itemize}
\subsubsection{Depth First Search}
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/27.PNG}
\end{center}
A volte, non abbiamo condizioni complesse sul percorso ma, semplicemente, il valore di una foglia (quindi lo stato attuale) ci basta per capire
se la condizione di interesse è soddisfatta. Allora, in questo caso, possiamo usare un algoritmo di ricerca in profondità (DFS).
una volta che l'algoritmo trova una soluzione, \textbf{esso termina}.
L'algoritmo, durante la sua esecuzione, \textbf{tiene in memoria solamente le informazioni relative alla soluzione}.
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/28.PNG}
\end{center}
le uniche cose che bisogna memorizzare quando si effettua una DFS su un albero di ricerca sono quindi \textbf{quali nodi sono attivi nella soluzione attuale} e quali figli
di un certo nodo sono \textbf{ancora da esplorare}.
Le proprietà dell'algoritmo sono le seguenti:
\begin{itemize}
    \item Quali sono i nodi che la DFS espande?
    \begin{itemize}
        \item Espande alcuni nodi dell'albero fino ad una profondità $m$
        \item Potrebbe processare l'intero albero
    \end{itemize}
    \item \textbf{Completezza}: Poiché $m$ può essere infinito, prevenire dei cicli \textbf{potrebbe aiutare a trovare una soluzione}
    \item \textbf{Ottimalità}: L'algoritmo non è ottimale, dato che trova la \textbf{soluzione più a sinistra}, senza curarsi della profondità o del costo 
\end{itemize}
In termini di complessità, se si hanno $b$ decisioni per stato e la massima profondità permessa è $m$ si ha che:
\begin{itemize}
    \item \textbf{Spazio}: $\mathcal{O}(bm)$
    \item \textbf{Tempo}: $\mathcal{O}(b^m)$ nel caso peggiore, ma potrebbe ridursi molto se le soluzioni sono semplici da trovare
\end{itemize}
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/30.PNG}
\end{center}
\subsubsection{Breadth First Search}
Una ricerca in ampiezza (BFS) andrà ad espandere \textbf{tutti i nodi dell'albero che si trovano allo stesso livello}. Così facendo, l'algoritmo
va a creare una \textbf{frontiera di nodi che devono essere ancora espansi}. Una volta terminati i nodi da espandere in un certo livello, l'algoritmo
passa ad espandere i nodi del livello successivo.
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/31.PNG}
\end{center}
procedendo in questo modo, l'algoritmo \textbf{riesce a trovare la soluzione di costo minimo} (o quella che richiede meno passi).
Le proprietà della BFS sono le seguenti:
\begin{itemize}
    \item Quali nodi espande una BFS?
    \begin{itemize}
        \item Processa tutti i nodi sopra la soluzione meno in profondità. Sia questa profondità $s$
    \end{itemize}
    \item \textbf{Completezza}: Se $s$ è finito, allora di sicuro una soluzione esiste. L'algoritmo è completo
    \item \textbf{Ottimalità}: è ottimo solo se i costi dei cammini sono tutti \textbf{uguali}
\end{itemize}
In termini di complessità, abbiamo che:
\begin{itemize}
    \item \textbf{Spazio}: $\mathcal{O}(b^s)$
    \item \textbf{Tempo}: $\mathcal{O}(b^s)$
\end{itemize}
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/32.PNG}
\end{center}
\subsubsection{DFS con approfondimento iterativo (IDDFS)}
L'idea di questo algoritmo è di combinare la complessità spaziale di una DFS con la complessità
temporale di una BFS. L'algoritmo quindi esegue i seguenti passi:
\begin{itemize}
    \item Lancia una DFS con limite di profondità 1. Se non trovi soluzioni... 
    \item Lancia una DFS con limite di profondità 2. Se non trovi soluzioni... 
    \item Lancia una DFS con limite di profondità 3. Se non trovi soluzioni... 
    \item ...
\end{itemize}
\begin{center}
    \includegraphics[width = 0.70\linewidth]{Images/32.PNG}
\end{center}
L'algoritmo, tuttavia, non è \textbf{estremamente ridondante?}
Generalmente, la maggior parte del lavoro avviene al \textbf{livello più in profondità esplorato},
quindi l'algoritmo non risulta ridondante. In termini computazionali, se si hanno $b$ possibili scelte per stato
e $s$ è la profondità della soluzione:
\begin{itemize}
    \item \textbf{Spazio}: $\mathcal{O}(s)$
    \item \textbf{Tempo}: $\mathcal{O}(b^s)$
\end{itemize}
\newpage \noindent
Inoltre:
\begin{itemize}
    \item \textbf{Completezza}: quando $s$ è finito
    \item \textbf{Ottimalità}: L'algoritmo è ottimale
\end{itemize}
ricapitolando:
\begin{center}
    \includegraphics[width = 0.50\linewidth]{Images/34.PNG}
\end{center}

\subsubsection{Il problema della tigre}
Un esempio, molto presente in letteratura, è il cosiddetto "problema della tigre";
ed esso è un esempio di problema \textbf{non risolvibile con gli algoritmi che abbiamo visto fino ad adesso}.
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/35.PNG}
\end{center}
il problema ha la seguente traccia:
\begin{center}
    Ci si trova in un labirinto formato da stanze. Ogni stanza è collegata alle stanze adiacenti tramite una porta.
    Dietro ad ogni porta ci può essere una tigre oppure delle caramelle, le quali sono il nostro obbiettivo.
    Dopo ogni mossa, la tigre si può muovere in una delle stanze adiacenti a quella in cui si trova correntemente.
    Quale sequenza di azioni bisogna compiere per raggiungere l'uscita del labirinto?
\end{center}
l'agente non sa se dietro una determinata porta c'è la tigre oppure le caramelle; per scoprilo esso può \textbf{ascoltare e sentire se dietro ad una porta sente il rumore provocato dalla tigre}.
Se assumiamo che \textbf{ad ogni stato corrisponda lo stesso risultato} e che quindi una soluzione al problema sia \textbf{un'associazione tra uno stato ed un'azione}; allora in ogni stato possiamo assumere
di conoscere \textbf{quale azioni risolve il problema dato}. In questo caso, tuttavia, \textbf{questa assunzione è errata!} Se, per esempio, nello stato 1 decidiamo di \textbf{ascoltare}, allora abbiamo prodotto
l'associazione "nello stato 1, ascolta"; quindi, se più avanti nella risoluzione del problema torniamo allo stato 1, allora \textbf{ci troveremo ad ascoltare di nuovo}, poiché ad ogni stato viene associata un'azione.
Quindi, quando le conseguenze delle azioni non dipendono solamente da ciò che si può osservare in un determinato istante, non possiamo usare \textbf{gli algoritmi di ricerca che abbiamo visto fino ad ora per risolvere il problema}.
In questo caso bisognerebbe cambiare l'insieme degli stati in modo da \textbf{conservare l'informazione di "aver ascoltato la tigre"}.
Quando andiamo a definire lo spazio degli stati per un problema, ci sono diverse difficoltà da considerare poiché abbiamo la necessità 
che la conseguenza di un'azione dipenda solamente dallo stato in cui si compie l'azione.
Se lo spazio degli stati \textbf{non descrive tutte le informazioni necessarie per risolvere il problema, allora gli algoritmi di ricerca potrebbero entrare in un loop infinito} invece
che convergere alla soluzione.
\subsubsection{Grafo dello spazio degli stati}
Lo spazio degli stati e la funzione di transizione di un problema di ricerca possono anche essere rappresentati
\textbf{mediante un grafo orientato}, che prende il nome di \textbf{grafo dello spazio degli stati}
\begin{center}
    \includegraphics[width = 0.50\linewidth]{Images/36.PNG}
\end{center}
ogni nodo di un grafo dello spazio degli stati rappresenta uno \textbf{stato}, mentre ogni arco rappresenta \textbf{l'azione che dobbiamo intraprendere per passare da un certo stato ad un altro}.
In un grafo dello spazio degli stati \textbf{non ci possono essere stati duplicati!}
Come per l'albero di ricerca, è impossibile, per problemi interessanti, rappresentare in memoria l'intero grafo degli stati.
Un albero di ricerca \textbf{rappresenta l'insieme di percorsi e le soluzioni che sono state studiate fino a quel momento}; una soluzione del problema è quindi un \textbf{cammino dalla radice alla foglia il cui stato soddisfa le condizioni del problema}.
In certi casi, una rappresentazione a grafo dello spazio degli stati è molto più conveniente di un albero, il quale potrebbe risultare di profondità \textbf{infinita}
\begin{center}
    \includegraphics[width = 0.65\linewidth]{Images/37.PNG}
\end{center}
tendenzialmente quindi, un albero di ricerca ha più nodi di un grafo dello spazio degli stati.
\subsubsection{Il problema del costo}
Come abbiamo visto sopra, un problema di ricerca è anche caratterizzato dal \textbf{costo che ogni azione ha per essere intrapresa}.
Come sappiamo, una \textbf{soluzione} è una sequenza di azione che raggiunge uno stato che \textbf{superi il "goal test"} (quindi una soluzione arriva ad una configurazione accettabile del problema).
Una \textbf{soluzione ottima} è la soluzione \textbf{con il costo complessivo minore}.
Poiché diverse azioni, che portano in diversi stati, possono avere diversi costi; cosa può succedere quindi quando si cerca di costruire
un \textbf{cammino di costo minimo} su un grafo che presenta \textbf{degli archi con costo negativo}?
Se esiste un ciclo sul grafo che presenta archi di costo negativo, allora si rischia \textbf{che il cammino di costo minimo abbia costo $-\infty$}.
Per i prossimi algoritmi che presenteremo, assumeremo quindi che ogni arco abbia un costo \textbf{positivo}.
Prima di presentarli facciamo due ulteriori considerazioni:
\begin{itemize}
    \item Un algoritmo che effettua una BFS trova il cammino di costo minimo se e solo se i costi di ogni azione sono uguali
    \item Un algoritmo che effettua una DFS trova il cammino di costo minimo se e solo se i costi di ogni azione sono nulli
\end{itemize}
\subsubsection{Uniform Cost Search (UCS)}
In questo algoritmo, si procede ad esplorare il nodo che ha \textbf{costo minore dall'origine}.
\begin{center}
    \includegraphics[width = 0.90\linewidth]{Images/38.PNG}
\end{center}
un implementazione di questo algoritmo è quindi la seguente:
\begin{center}
    \includegraphics[width = 0.85\linewidth]{Images/39.PNG}
\end{center}
questo algoritmo ha le seguenti proprietà:
\begin{itemize}
    \item Quali nodi espande l'algoritmo?
    \begin{itemize}
        \item Espande tutti i nodi con \textbf{costo minore rispetto alla soluzione meno costosa!}
        \item Se la soluzione costa $C^*$ e gli archi hanno un costo di almeno $\varepsilon$, allora "la profondità effettiva" della soluzione meno costosa è $C^*/\varepsilon$.
    \end{itemize}
    \item \textbf{Completezza}: Assumendo che $C^*$ è finito e $\varepsilon > 0$, allora si!
    \item \textbf{Ottimalità}: L'algoritmo è ottimale
\end{itemize}
In termini computazionali, si ha che, se $b$ è il numero di azioni possibili in ogni stato:
\begin{itemize}
    \item \textbf{Spazio}: $\mathcal{O}(b^{C^*/\varepsilon})$
    \item \textbf{Tempo}: $\mathcal{O}(b^{C^*/\varepsilon})$
\end{itemize}
\begin{center}
    \includegraphics[width = 0.55\linewidth]{Images/40.PNG}
\end{center}
Enunciamo il teorema di \textbf{correttezza} per questo algoritmo:
\begin{Teorema}[Correttezza di UCS]
    Quando uno stato $s$ viene rimosso dalla frontiera e posto tra i nodi esplorati, la sua priorità è $PastCost(s)$, cioè il minimo costo di $s$
\end{Teorema}
\begin{Dimostrazione}
    Se $s$ viene scelto, allora è il nodo che ha \textbf{costo minimo sulla frontiera}; quindi ogni altro cammino ad $s$ sarà più costoso, se i costi sono positivi.
    Per ogni nodo $u$ nella frontiera, si ha che:
    $$PastCost(s) \leq PastCost(u) \leq PastCost(u) + Cost(u,s)$$
    \begin{center}
        \includegraphics[width = 0.55\linewidth]{Images/41.PNG}
    \end{center}
\end{Dimostrazione}
Tutti gli algoritmi che abbiamo visto fino ad ora sono \textbf{uguali} tranne per \textbf{la gestione dei nodi sulla frontiera}:
\begin{itemize}
    \item Concettualmente, tutte le frontiere sono code di priorità (cioè, collezioni di nodi a cui viene attaccata una certa priorità)
    \item Praticamente, per DFS e BFS, si può evitare l'overhead di $log(n)$ dato da una priority queue usando stack e code
    \item Si può addirittura implementare una variante che prende in input "oggetto di accodamento" (variabile)
\end{itemize}
\subsubsection{Ricerca informata}
Il problema principale di UCS è che \textbf{esplora l'albero degli stati in ogni direzione}; quindi non tiene conto di nessuna informazione
riguardante la \textbf{posizione dell'obbiettivo all'interno dell'albero}.
\begin{center}
    \includegraphics[width = 0.40\linewidth]{Images/42.PNG}
\end{center}
per velocizzare il processo di ricerca, dobbiamo quindi \textbf{guidare la ricerca verso l'obbiettivo}, invece che cercare in tutte le direzioni possibili.
Questo tipo di ricerca viene detto \textbf{ricerca informata}
\begin{center}
    \includegraphics[width = 0.45\linewidth]{Images/43.PNG}
\end{center}
per poter far questo, abbiamo bisogno di inserire più informazione.
In questo caso, tuttavia, l'informazione aggiuntiva non riguarda delle caratteristiche dell'oggetto ma è una \textbf{semplice funzione euristica} che
indica "quanto è facile raggiungere lo stato un obbiettivo da un certo stato".
Queste funzioni euristiche sono \textbf{funzioni dell'obbiettivo}, quindi, per ogni obbiettivo da raggiungere, necessitiamo di una funzione euristica.
Quindi, \textbf{un euristica} è:
\begin{itemize}
    \item Una \textbf{funzione} che \textit{stima} quanto vicino è uno stato all'obbiettivo
    \item Ogni euristica è progettata per un particolare problema di ricerca
    \item Esempi: Manhattan Distance, Euclidean Distance for pathing ecc...
\end{itemize}
Se l'agente impara una certa funzione euristica per un certo obbiettivo; se l'obbiettivo cambio sarà necessario apprendere
una nuova euristica. Un euristica, per funzionare, deve però \textbf{effettuare previsioni minori del valore reale}, cioè deve sottostimare
il costo di raggiungere l'obbiettivo rispetto al costo reale.
\subsubsection{Greedy Search}
Uno degli approcci che utilizza le euristiche viene chiamato \textbf{greedy search}.
L'algoritmo andrà ad esplorare il nodo \textbf{che sembra essere più vicino all'obbiettivo}
\begin{center}
    \includegraphics[width = 1\linewidth]{Images/44.PNG}
\end{center}
l'algoritmo è quindi simile ad UCS, ma al posto di esplorare il nodo che ha costo minimo rispetto all'origine,
\textbf{esplora invece il prossimo nodo che ha un costo atteso, quindi un'euristica minima, verso l'obbiettivo}.
La strategia dell'algoritmo è quindi quella di espandere il nodo che \textbf{l'euristica valuta come più vicino all'obbiettivo} (quindi il migliore tra i nodi da esplorare).
L'euristica in questo caso offre, \textbf{per ogni stato dell'albero}, una \textbf{stima della distanza dall'obbiettivo più vicino}.
\begin{center}
    \includegraphics[width = 0.40\linewidth]{Images/45.PNG}
\end{center}
L'algoritmo tuttavia, \textbf{in presenza di un euristica non ottimale}, fatica nel trovare effettivamente una soluzione ottima \textbf{poiché esso non tiene conto del costo totale per raggiungere ogni nodo a partire dalla radice} ma
considera solamente il costo \textbf{per passare da un nodo all'altro}.
\begin{center}
    \includegraphics[width = 0.40\linewidth]{Images/46.PNG}
\end{center}
\subsubsection{$A^*$ Search}
L'algoritmo di ricerca $A^*$ combina gli algoritmi UCS e Greedy Search.
Per farlo, assegna ad ogni nodo dell'albero un numero ottenuto sommando i costi del percorso seguito per arrivare fino a quel nodo e la stima del costo che
l'euristica prevede per arrivare all'obbiettivo da quel nodo.
Siano quindi:
\begin{itemize}
    \item $g(n)$ il costo per arrivare dalla radice al nodo $n$
    \item $h^*(n)$ il costo ottimale per arrivare da $n$ all'obbiettivo più vicino
\end{itemize}
L'idea alla base di $A^*$ è quindi la seguente:
\begin{itemize}
    \item Si espande un nodo $n$ che ha maggiore probabilità di essere su un cammino ottimale verso l'obbiettivo
    \item Si espande un nodo $n$ tale che il costo della migliore soluzione che passa per $n$ sia ottimale
    \item Si espande un nodo $n$ con il minor valore di $g(n) + h^*(n)$ 
    \item Solo in rari casi sappiano quanto vale $h^*(n)$, tuttavia potremmo avere una sua \textbf{approssimazione euristica} $h(n)$
    \item $A^* =$ ricerca su un albero con una coda di priorità ordinata rispetto a $f(n) = g(n) + h(n)$
\end{itemize}
\begin{center}
    \includegraphics[width =1\linewidth]{Images/47.PNG}
\end{center}
Per fare in modo che \textbf{$A^*$ sia ottimo}, abbiamo bisogno che l'euristica sia \textbf{ottimista rispetto al costo reale}.
Quando si dovrebbe fermare l'algoritmo? Ovviamente, \textbf{non possiamo fermarci quando troviamo il goal} (cioè quando immettiamo nella coda di priorità il nodo), poiché
\textbf{potrebbero esistere dei cammini sul grafo di costo minore di quello correntemente considerato}.
Ci fermiamo quindi solamente quando \textbf{rimuoviamo dalla coda di priorità il nodo goal} (cioè quando lo espandiamo).
In che condizioni $A^*$ è \textbf{ottimale}? Per fare in modo che $A^*$ trovi il cammino di costo minimo, l'euristica che esso usa
deve essere \textbf{ammissibile}:
\begin{itemize}
    \item Un'euristica \textbf{inammissibile} (pessimistica) rompe l'ottimalità dell'algoritmo, poiché non gli permette di esplorare le soluzioni che porterebbero alla soluzione ottima
    \item Un'euristica \textbf{ammissibile} (ottimista) non è mai al di sopra del costo reale
\end{itemize}
Diamo quindi una definizione di euristica ammissibile:
\begin{Definizione}[Euristica ammissibile]
    Un'euristica ammissibile $h$ si dice ammissibile quando
    $$0 \leq h(n) \leq h^*(n)$$
    dove $h^*(n)$ è il reale costo per arrivare all'obbiettivo più vicino
\end{Definizione}
Enunciamo quindi il seguente teorema:
\begin{Teorema} [Ottimalità di $A^*$]
    Si assuma che $A$ sia un noto obbiettivo ottimo, che $B$ sia un nodo obbiettivo sub-ottimale e che $h$ sia un'euristica ammissibile.
    Allora $A$ sarà scelta per l'espansione prima di $B$
\end{Teorema}
\begin{Dimostrazione}
    Immaginiamo che sia $B$ sia alcuni antenati $n$ di $A$ stiano sulla frontiera (oppure che sulla frontiera ci sia $A$ stesso).
    Dobbiamo dimostrare quindi che $n$ sarà scelto per l'espansione prima di $B$.
    Facciamo le seguenti considerazioni:
    \begin{enumerate}
        \item $f(n) \leq f(A)$ poiché:
        \begin{itemize}
            \item La definizione della funzione di costo indica che:
            $$f(n) = g(n) + h(n)$$
            $$f(A) = g(A) + h(A)$$
            \item L'euristica ammissibile sottostima il vero costo di A, quindi $h(A) = 0$
            \item $h(n)$ deve sottostimare il vero costo per arrivare da $n$ ad $A$, quindi $g(n) + h(n) \leq g(A)$,
            quindi:
            $$f(n) \leq f(A)$$
        \end{itemize}
        \item $f(A) < f(B)$ poiché:
        \begin{itemize}
            \item Sappiamo che:
            $$f(A) = g(A) + h(A)$$
            $$f(B) = g(B) + h(B)$$
            \item L'euristica deve sottostimare i veri costi per arrivare ad A e B, quindi:
            $$h(A) = h(B) = 0$$
            \item Poiché abbiamo assunto che $B$ è un obbiettivo sub-ottimale, allora sicuramente:
            $$g(A) < g(B) \Rightarrow f(A) < f(B)$$
        \end{itemize}
    \end{enumerate}
    Quindi tutti gli antenati di $A$ saranno espansi prima di $B$ e $A$ sarà espanso prima di $B$.
    L'algoritmo $A^*$ è quindi ottimo.
\end{Dimostrazione}
\subsubsection{Creare euristiche ammissibili}
Spesso, le euristiche ammissibili non sono altre che \textbf{soluzioni a rilassamenti di un problema}, dove nuove azioni vengono rese ammissibili.
Diamo quindi la seguente definizione:
\begin{Definizione}[Problema rilassato]
    Un problema $P_2$ è una versione rilassata del problema $P_1$ se $\mathcal{A}_2(s) \supseteq \mathcal{A}_1(s)$ per ogni $s$
\end{Definizione}
\begin{Teorema}
    $h_2^*(s) \leq h_1^*(s)$ per ogni $s$, quindi $h_2^*(s)$ è un euristica ammissibile per $P_1$
\end{Teorema}
Quali fattori quindi possiamo considerare quando cerchiamo una buona euristica?
\begin{itemize}
    \item La funzione di successione aiuta? E le azioni?
    \item Cosa succede se si aggiungono/rimuovono casualmente delle azioni da stati casuali? Quali sarebbero le soluzioni risultanti in una funzione euristica ammissibile?
\end{itemize}
Quando si hanno più euristiche tra cui scegliere, esse si possono \textbf{combinare}.
Quali sono quindi i criteri da seguire quando si combinano delle euristiche?
\begin{itemize}
    \item \textbf{Dominanza}: $h_1 \geq h_2$ se $\forall n, h_1(n) \geq h_2(n)$
    Quindi, assumendo che entrambe le euristiche siano ammissibili, si prende quella con il valore più ottimista.
    \item L'euristica nulla non è per niente ottimale (Con $A^*$, ci si ritrova nuovamente con UCS)
    \item L'euristica esatta è buona, ma solitamente troppo costosa
    \item \textbf{Cosa fare se nessuna delle due euristiche domina l'altra?}
    \begin{itemize}
        \item Si forma una nuova euristica prendendo i valori massimi di entrambe:
        $$h(n) = \max\{h_1(n), h_2(n)\} \; \forall n$$
        Il massimo di due euristiche ammissibili è esso stesso ammissibile e domina entrambe.
    \end{itemize}
\end{itemize}
\subsubsection{Euristiche consistenti}
Una ricerca DFS ha il problema che \textbf{si blocca in caso di loop} all'interno dell'albero.
Tuttavia, una DFS risparmia memoria ed è possibile riutilizzarla in altri contesti.
Vogliamo quindi evitare che la DFS espanda gli stessi sotto-alberi.
Come possiamo fare? \textbf{Abbiamo abbastanza informazioni solamente guardando l'albero di ricerca per farlo?}
La strategia di selezione che abbiamo seguito fino ad ora è quella di partire ad espandere i nodi dell'albero che non sono ancora stati espansi;
tuttavia non abbiamo controllato se \textbf{questi nodi sono già presenti nell'albero}. Dobbiamo quindi gestire meglio la struttura dati che utilizziamo
per implementare l'algoritmo.
\begin{center}
    \includegraphics[width =0.90\linewidth]{Images/48.PNG}
\end{center}
Ciò che si fa per evitare questa problematica è passare da algoritmi di ricerca sugli alberi a \textbf{algoritmi di ricerca su grafi}.
L'idea è quindi quella di \textbf{non espandere mai uno stato due volte}.
L'implementazione è quindi la seguente:
\begin{itemize}
    \item Alla ricerca sull'albero si aggiunge un \textbf{insieme degli stati già espansi} ("closed set")
    \item Si espande l'albero di ricerca nodo per nodo, ma... 
    \item Prima di espandere un nodo, ci si assicura che quello stato non sia mai stato espanso prima
    \item Se lo stato è già stato espanso, lo si salta, altrimenti lo si aggiunge al closed set
\end{itemize}
\textbf{IMPORTANTE}: Bisogna immagazzinare il closed set \textbf{come un set}, non una lista. \newline
Questo approccio riesce quindi ad evitare la presenza di loop, tuttavia esso può \textbf{andare a intaccare la completezza e l'ottimalità degli algoritmi di ricerca}.
Facciamo il seguente esempio:
\begin{center}
    \includegraphics[width =0.90\linewidth]{Images/49.PNG}
\end{center}
come si può vedere, il percorso di costo minimo verrebbe bloccato. Per evitare questo tipo di problematica, abbiamo quindi bisogno di un'\textbf{euristica consistente}:
\begin{itemize}
    \item \textbf{Idea principale}: La stima euristica dei costi deve essere minore o uguale al costo effettivo
    \begin{itemize}
        \item \textbf{Ammissibilità}: Il costo euristico deve essere minore del costo effettivo per arrivare all'obbiettivo:
        $$h(A) \leq h^*(A)$$
        \item \textbf{Consistenza}: Il costo euristico per ogni arco deve essere minore o uguale al costo effettivo per ogni arco:
        $$h(A) - h(C) \leq c(A \; to \; C)$$
        oppure deve rispettare la diseguaglianza triangolare:
        $$h(A) \leq c(A \; to \; C) + h(C)$$
        \textbf{Nota}: $h^*$ necessariamente soddisfa la diseguaglianza triangolare
    \end{itemize}
    \item \textbf{Conseguenze della consistenza}:
    \begin{itemize}
        \item Il valore di $f$ lungo un certo cammino non decresce mai:
        $$h(A) \leq c(A, C) + h(C) \Rightarrow g(A) + h(A) \leq g(A) + c(A,C) + h(C)$$
        \item $A^*$ quindi è ottimale
    \end{itemize}
\end{itemize}
Rendere un euristica consistente però \textbf{non è banale}, poiché bisognerebbe verificarla per tutti gli stati; non possiamo quindi
farlo tramite un approccio "forza bruta", poiché richiederebbe calcolare \textbf{il percorso ottimo all'obbiettivo da ogni singolo stato}.
Quindi, se si ha un'euristica consistente, allora si può direttamente utilizzare $A^*$.
Altrimenti, è necessario avere dei controlli più complessi su quali stati sono già stati visitati.
Per quanto riguarda l'ottimalità di $A^*$ quindi:
\begin{itemize}
    \item \textbf{Tree search}: $A^*$ è ottimo se l'euristica è \textbf{ammissibile}
    \item \textbf{Graph search}: $A^*$ è ottimo se l'euristica è \textbf{consistente}
    \item La consistenza di un'euristica ne implica l'ammissibilità
    \item La maggior parte delle euristiche ammissibili tendono ad essere anche consistenti, specialmente se esse derivano da problemi rilassati
\end{itemize}
Tuttavia, dobbiamo tenere in considerazione che:
\begin{itemize}
    \item $A^*$ mantiene l'intera regione esplorata in memoria, quindi finirà lo spazio per problemi molto complessi
    \item Ci sono varianti che utilizzano meno memoria:
    \begin{itemize}
        \item $IDA^*$ funzione come IDDFS, eccetto che usa una funzione di limite $f$ invece che un limite di profondità
        \item $RBFS$ è un algoritmo DFS ricorsivo che usa una funzione di limite $f$ uguale al miglior cammino alternativo disponibile da uno qualsiasi degli antenati del nodo corrente
        \item $SMA^*$ usa tutta la memoria disponibile per la coda, minimizza quindi il thrashing.
    \end{itemize}
\end{itemize}
\subsection{Classical Planning}
Fino ad adesso, non abbiamo utilizzato tutte le caratteristiche degli stati. Infatti, abbiamo trattato
gli stati come se fossero delle "etichette" per capire se un certo stato fosse già stato esplorato o meno.
I problemi di \textbf{planning} sono simili ai problemi di ricerca nella misura che, per entrambi, \textbf{una soluzione è un insieme di passi sequenziali che permetta di raggiungere uno stato desiderato}.
Un \textbf{planner} accetta un problema e automaticamente computa una sua soluzione. Per farlo, i problemi devono essere codificati in un \textbf{linguaggio indipendente dal dominio} (domain-independent language):
\begin{itemize}
    \item Quali \textbf{azioni} e \textbf{sensori} sono disponibili e come funzionano
    \item Qual'è \textbf{l'obbiettivo} e qual'è \textbf{la situazione iniziale}
\end{itemize}
Nel caso più semplice, abbiamo:
\begin{enumerate}
    \item Una situazione iniziale \textbf{completamene nota}
    \item Gli effetti delle azioni sono \textbf{deterministici}
    \item Non c'è bisogno di \textbf{alcuna percezione}
\end{enumerate}
Questa forma di planning è detta, nel campo dell'IA, \textbf{classical planning}.
Nel caso più generale, le assunzioni sopra vengono \textbf{rilassate}.
In un problema di classical planning, abbiamo:
\begin{itemize}
    \item Uno \textbf{spazio degli stati} discreto e finito $S$
    \item Uno \textbf{stato iniziale noto} $s_0 \in S$
    \item Un insieme $S_G \subseteq S$ di \textbf{stati obbiettivo}
    \item Delle azioni $\mathcal{A}(s) \subseteq A$ applicabili ad ogni $s \in S$
    \item Una \textbf{funzione di transizione degli stati deterministica} $s^j = f(a,s) \; per \; a \in \mathcal{A}(s)$
    \item Un \textbf{costo positivo} per le azioni $c(a, s)$
\end{itemize}
Una \textbf{soluzione} (o \textbf{piano}) è una sequenza di azioni applicabili $a_0, \dots, a_n$ che mappa $s_0$ all'interno di $S_G$, cioè
esiste una sequenza di stati $s_0, \dots, s_{n+1}$ tale che $a_i \in \mathcal{A}(s_i),$ \newline  $s_{i+1} = f(a_i, s_i)$, e $s_{n+1} \in S_G, i = 0,\dots,n$.
Un piano è \textbf{ottimale} se minimizza la \textbf{somma dei costi delle azioni}, cioè
$$\sum_{i=0}^n c(a_i, s_i)$$
Tuttavia, modellare un problema secondo un modello a stati e risolverlo come un problema di ricerca è \textbf{differente} rispetto a modellare il problema come un problema di classical planning.
La differenza viene dall'uso dei \textbf{linguaggi di planning} (STRIPS, PDDL, ecc...) e da una \textbf{rappresentazione più ricca degli stati} (data proprio da questi linguaggi), i quali, in un problema di classical planning, non sono solo
delle label. Si passa inoltre da agenti che hanno una conoscenza basilare dell'ambiente ad \textbf{agenti con una conoscenza dell'ambiente più strutturata}:
\begin{itemize}
    \item Gli agenti \textbf{acquisiscono conoscenza attraverso la percezione, \newline l'apprendimento  e il linguaggio}
    \begin{itemize}
        \item Gli agenti possono rappresentare \textbf{conoscenza sulle conseguenze delle proprie azioni}, quindi si possono utilizzare svariati linguaggi (LISP, Prolog, Logiche di vario tipo, ...) per rappresentare la funzione di transizione ("transition model")
        \item Gli agenti sanno come i loro sensori \textbf{corrispondo allo stato del mondo} ("sensor model"), cioè hanno conoscenza sul come interpretare le percezioni dei propri sensori
        \item Gli agenti conoscono lo \textbf{stato attuale dell'ambiente}
    \end{itemize}
    \item Gli agenti \textbf{possono anche tener traccia delle parti dell'ambiente che attualmente non sta osservando}, quindi possono tener traccia di \textbf{un ambiente parzialmente osservabile}. I linguaggi di planning quindi possono esprimere condizioni più complesse, dove vi è una distinzione fra quello che l'agente osserva in un'istante  e quello che è effettivamente lo stato del mondo.
    \item Gli agenti \textbf{possono formulare piani per raggiungere l'obbiettivo}
\end{itemize}
Gli agenti quindi si avvalgono di una \textbf{base di conoscenza}, cioè di un'\textbf{insieme di frasi scritte in un linguaggio formale}.
La costruzione di un'agente avviene quindi in \textbf{maniera dichiarativa}:
\begin{itemize}
    \item Bisogna \textbf{dirgli oppure fargli apprendere} ciò che ha bisogno di conoscere
    \item Dopodiché, esso può \textbf{chiedersi} cosa fare: le risposte dovrebbero \textbf{derivare dalla base di conoscenza} (KB) 
\end{itemize}
Gli agenti possono essere visti, quindi, \textbf{al livello della conoscenza}, cioè ci si può focalizzare \textbf{su ciò che sanno} indipendentemente da come essi sono implementati.
Un singolo algoritmo di inferenza può rispondere a tutte le domande a cui è possibile rispondere.
\begin{center}
    \includegraphics[width =0.60\linewidth]{Images/50.PNG}
\end{center}
Facciamo un esempio: supponiamo di voler codificare in logica proposizionale una partita di PacMan sulla seguente mappa:
\begin{center}
    \includegraphics[width =0.30\linewidth]{Images/51.PNG}
\end{center}
In questo caso, la maggior parte della base di conoscenza sarebbe occupata da $\mathcal{O}(NT)$ frasi del modello di transizione, con $N$ il numero
di posizioni possibili e $T$ gli istanti temporali. Se ogni frase sono più o meno 10 linee di testo, e se supponessimo $N = 200, T = 400$ allora dovremmo scrivere, più o meno,
800.000 linee di testo, cioè 20.000 pagine.
Questo è dato dal fatto che la logica proposizionale ha una capacità espressiva limitata.
Invece, se decidessimo di usare la logica del primo ordine, avremmo bisogno solamente di un numero $\mathcal{O}(1)$ di frasi del modello di transizione.
Tuttavia, se decidessimo comunque di utilizzare la logica proposizionale, potremmo scrivere un \textbf{agente basato sulla conoscenza} che, ad ogni passo, aggiorni la base di conoscenza e si chieda qual'è la prossima azione da compiere (quindi effettua un'inferenza sulla KB):
\begin{center}
    \includegraphics[width =0.70\linewidth]{Images/52.PNG}
\end{center}
questo tipo di approccio può essere usato anche per i seguenti problemi:
\begin{itemize}
    \item \textbf{Localizzazione} con una mappa e delle percezioni locali 
    \begin{itemize}
        \item Dati una KB iniziale, più una sequenza di azioni e percezioni, dove sono?
    \end{itemize}
    \item \textbf{Costruire una mappa} con un sensore locale
    \begin{itemize}
        \item Data una KB iniziale, più una sequenza di azioni e percezioni, qual'è la mappa?
    \end{itemize}
    \item \textbf{SLAM: Simultaneous Localization and Mapping}
    \begin{itemize}
        \item Localizzazione + Costruire una mappa
    \end{itemize}
    \item \textbf{Planning}
    \begin{itemize}
        \item Data una KB iniziale, più una sequenza di azioni e percezioni, qual'è la sequenza di azioni che mi garantisce di raggiungere l'obbiettivo?
    \end{itemize}
\end{itemize}
Il vantaggio di questo approccio è che, per tutti i problemi precedenti, \textbf{possiamo usare lo stesso algoritmo e la stessa KB}.
Ricapitolando, quindi:
\begin{itemize}
    \item Una possibile architettura par un agente è quella di conoscenza + inferenza
    \item La logica mette a disposizione una maniera formale per codificare la conoscenza
    \item Una semplice KB per PacMan compre lo stato iniziale, il sensor model e il transition model
    \item L'inferenza logica computa le \textbf{relazioni di implicazione} tra le frasi, permettendo che un ampio spettro di compiti possano essere risolti
\end{itemize}
\subsubsection{STRIPS}
STanford Research Institute Problem Solver (STRIPS) è un linguaggio di classical planning.
Un \textbf{problema} in STRIPS è una tupla $P = (F, O, I ,G)$ con:
\begin{itemize}
    \item $F$ l'insieme di tutti gli \textbf{atomi} (variabili booleane)
    \item $O$ l'insieme di tutti gli \textbf{operatori} (azioni)
    \item $I \subseteq F$ l'insieme delle \textbf{situazioni iniziali}
    \item $G \subseteq F$ l'insieme delle \textbf{situazioni obbiettivo}
\end{itemize}
Gli operatori $o \in O$ sono rappresentati combinando:
\begin{enumerate}
    \item l'operazione di aggiunta alla lista degli atomi veri $Add(o) \subseteq F$ (simile alla notazione dei DB relazionali. Se è nella lista, allora è vero)
    \item L'operazione di rimozione dalla lista degli atomi veri $Del(o) \subseteq F$
    \item La lista delle precondizioni $Pre(o) \subseteq F$
\end{enumerate}
Un problema in STRIPS $P$ determina uno \textbf{modello a stati} $S(P)$ dove:
\begin{itemize}
    \item Gli stati $s \in S$ sono \textbf{collezioni di atomi} da $F$
    \item Lo stato iniziale $s_0$ è $I$
    \item Gli stati obbiettivo $s$ sono tali che $G \subseteq s$
    \item Le azioni $a \in \mathcal{A}(s)$ sono operazioni in $O$ tali che $Pre(a) \subseteq s$
    \item Il prossimo stato $s^j = s - Del(a) + Add(a)$
    \item Il costo delle azioni $c(a,s)$ sono tutte 1
\end{itemize}
La \textbf{soluzione (ottima) del problema P} è una \textbf{soluzione (ottima) del problema $S(P)$}.
Le estensioni del linguaggio (negazione, effetti condizionali ecc...) offrono maniere convenienti per esprimere i problemi; alcune sono necessarie per descrivere modelli più ricchi (costi, probabilità, ...).
STRIPS ci permette di descrivere un problema \textbf{indipendentemente dal tempo}.
La lista degli atomi veri definisce, quindi, \textbf{lo stato del problema in un determinato istante}.
La \textbf{lista delle precondizioni} per una determinata azione, invece, definisce quali atomi devono essere veri per poter eseguire l'azione.
Facciamo un esempio:
\begin{center}
    \includegraphics[width =1\linewidth]{Images/53.PNG}
\end{center}
Vediamo ora un'esempio più complesso: supponiamo che un robot si trovi in un ambiente formato da due stanze.
Il robot deve trasportare due pacchi dalla stanza $A$ alla stanza $B$. La codifica in STRIPS del problema è quindi la
seguente:
\begin{center}
    \includegraphics[width =1\linewidth]{Images/54.PNG}
\end{center}
In questo caso, abbiamo quindi un numero di stati possibili di $2^8$.
Tuttavia, questa codifica presenta molte ripetizioni. Possiamo quindi usare delle \textbf{variabili} per 
definire in modo più compatto il problema. L'introduzione di variabili necessita inoltre che si possano definire
i \textbf{tipi di queste variabili}. Il problema quindi diventa:
\begin{center}
    \includegraphics[width =1\linewidth]{Images/55.PNG}
\end{center}
\newpage
\subsubsection{PDDL}
Il \textbf{Planning Domain Description Language} (PDDL) è una versione avanzata di STRIPS e si configura come una sintassi standard per i problemi di classical planning.
Sviluppato per la International Planning Competition(IPC), è un linguaggio che continua ad evolversi sin dalla sua creazione, nel 1998.
Durante l'IPC, i pianificatori vengono valutati su \textbf{problemi da loro non ancora visti} codificati in PDDL
\begin{center}
    \includegraphics[width =0.50\linewidth]{Images/56.PNG}
\end{center}
PDDL specifica una sintassi per i problemi $P = (F,I,O,G)$ e supporta STRIPS, variabili, tipi e molto altro.
I problemi in PDDL sono specificati in due parti:
\begin{itemize}
    \item \textbf{Dominio}: Contiene azioni e gli \textbf{schemi degli atomi}, oltre che ai \textbf{tipi degli argomenti}
    \item \textbf{Istanza}: Contiene la \textbf{situazione iniziale, l'obbiettivo} e le \textbf{costanti} (oggetti) per ogni tipo
\end{itemize}
Queste due parti, in PDDL, si trovano in due file separati, in modo da facilitare \textbf{la creazione di diverse istanze per lo stesso problema}.
\begin{center}
    \includegraphics[width =1\linewidth]{Images/57.PNG}
\end{center}
In PDDL, \textbf{i parametri di un'azione devono coprire sia le precondizione che gli effetti}; quindi sono le variabili che vanno utilizzate dentro un'azione
per crearne una completa.
Come facciamo però a risolvere un problema di classical planning?
Il planning è una delle aree più vecchie dell'IA; quindi molte idee sono state provate.
Ciò che si può fare, utilizzando questi linguaggi, è impiegare una vasta gamma di ragionatori per risolvere il problema; questo insieme è più ricco rispetto
a quello dei ragionatori che effettuano una ricerca su grafo poiché i primi possono sfruttare la struttura dello spazio degli stati.
I due metodi classici principali per risolvere problemi di classical planning sono:
\begin{itemize}
    \item Pianificazione come \textbf{ricerca euristica}; questo metodo è ispirato a ciò che abbiamo già detto sulle euristiche
    \item Pianificazione come \textbf{SAT}, cioè si trasforma il piano in un problema di soddisfacimento di vincoli, dove bisogna riuscire a definire il valore di tutte le variabili indefinite.
\end{itemize}
Questi metodi sono in grado di risolvere problemi caratterizzati da un spazio degli stati di dimensione considerevole.
Ovviamente, alcuni problemi sono intrinsecamente difficili, e quindi, per essi, i \textbf{pianificatori generali e indipendenti dal dominio} non saranno
probabilmente in grado di raggiungere le performance di \textbf{metodi specializzati}.
\subsubsection{Pianificazione come ricerca euristica}
L'idea della pianificazione come ricerca euristica è quella di \textbf{sfruttare la corrispondenza tra i modelli a stati (classici) e i grafi orientati}:
\begin{itemize}
    \item I \textbf{nodi} del grafo rappresentano gli \textbf{stati} $s$ del modello
    \item Gli archi $(s, s^j)$ catturano la transizione corrispondente nel modello con lo stesso costo
\end{itemize}
Nella formulazione di una pianificazione come ricerca euristica, il problema $P$ è risolto da degli \textbf{algoritmi di path-finding} su un grafo
associato con il modello $S(P)$, con un'\textbf{euristica} derivata automaticamente da $P$.
Uno sviluppo chiave nel campo del planning, avvenuto negli anni '90, è l'estrazione automatica di \textbf{funzioni euristiche} per guidare la ricerca dei piani (quindi di una soluzione).
L'euristica verrà derivata dal \textbf{rilassamento del problema} dove le \textbf{azioni di cancellazione dalla lista non hanno alcun effetto}. Viene quindi
eliminata la possibilità di rendere falso un atomo. Questo rilassamento viene detto \textbf{delete-relaxation}.
Siano quindi $P(s)$ il problema $P$ con $s$ come suo stato iniziale, $P^+$ la sua delete-relaxation e $h^*_P(s)$ il \textbf{costo ottimale} per risolvere $P$ da $s$.
Allora, \textbf{l'euristica} $h(s)$ potrebbe essere impostata a:
$$h(s) := h_P^+(s)$$
tuttavia, questo non \textbf{funziona computazionalmente}: risolvere $P^+(s)$ in maniera ottimale è difficile quanto risolvere $P(s)$ in maniera ottimale (il problema è NP-Completo).
Dall'altra parte, seppur risolvere il rilassamento $P^+(s)$ ottimamente sia difficile, \textbf{trovare una sua soluzione, non necessariamente ottimale, è semplice}.
Poiché in questo caso viene rimossa la possibilità di rimuovere atomi dalla lista, ciò che si può fare è eseguire \textbf{tutte le azioni eseguibili in un determinato momento} e quindi
vedere quali sono gli \textbf{stati raggiungibili al prossimo passo} e così via fino a non ottenere tutti gli elementi del goal nella lista.
Ciò quindi da una misura della distanza tra un certo stato e lo stato finale; che è un euristica \textbf{ottimistica}. Si può quindi prendere quest'euristica e usarla
per effettuare una ricerca informata (es. con $A^*$).
In realtà, la procedura che effettivamente si adopera comprende anche una fase di "back-tracking", dove si vanno a cancellare \textbf{tutte le azioni inutili}, andando quindi a semplificare
lo spazio degli stati. Un'euristica ammissibile per $h(s)$ per il problema rilassato $P^+$ è quindi l'euristica $h_{max}$, definita come segue:
\begin{itemize}
    \item \textbf{Idea chiave}: I problemi STRIPS \textbf{delete-free} sono \textbf{completamente decomponibili}
    \item Se il piano $\pi_1$ arriva al goal $G_1$ e il piano $\pi_2$ arriva al goal $G_2$, allora il piano $\pi_1, \pi_2$, dove $\pi_2$ segue $\pi_1$, risolve sia il goal $G_1$ che il goal $G_2$
    \item L'osservazione suggerisce la seguente \textbf{procedura iterativa} sopra tutti gli atomi in $P^+(s)$:
    \begin{enumerate}
        \item L'atomo $p$ è \textbf{raggiungibile} in 0 passi se $p \in s$
        \item L'atomo $p$ è \textbf{raggiungibile} in $i + 1$ passi se non è raggiungibile in $i$ passi o meno, ed esiste \textbf{un'azione} $a_P$ in $P$ tali che le \textbf{precondizioni} $p_i$ di $a_P$ sono raggiungibili in $i$ passi o meno        
    \end{enumerate}
    \item \textbf{Affermazioni}:
    \begin{itemize}
        \item La procedura sopra riportata termina in un numero di passi limitato dal numero di atomi; cioè quando non vi sono più atomi raggiungibili
        \item Se l'atomo $p$ non è raggiungibile, allora si può mostrare che non esiste un piano per raggiungere $p$ in $P$
    \end{itemize}
    \item \textbf{Proprietà}:
    \begin{itemize}
        \item $h(s)$ posta al numero di passi richiesti per raggiungere \textbf{tutti gli atomi obbiettivo} è un'\textbf{euristica ammissibile}. Viene chiamata \textbf{euristica massima} oppure $h_{max}$
    \end{itemize}
\end{itemize}
L'euristica $h = h_{max}$ è \textbf{ammissibile}, ma non è particolarmente \textbf{informativa}.
Anche l'euristica $h(s) = 0$ è \textbf{ammissibile}, ma è completamente \textbf{poco informativa}.
Delle euristiche più informative possono essere ottenute da un \textbf{piano} $\pi(s)$ per il rilassamento $P^+(s)$, che viene chiamato \textbf{piano rilassato}.
Un piano rilassato $\pi(s)$ viene ottenuto \textbf{concatenando a ritroso} partendo dagli obbiettivi e utilizzando le azioni $a_P$, identificate dalla \textbf{procedura di raggiungibilità}, che rendono raggiungibili i goal.
La procedura di raggiungibilità è definita come segue:
\begin{center}
    $\pi(s)$ contiene l'azione $a_p$ per ogni obbiettivo $p$ in $P$, $p \in s$, e un'azione $a_q$ per ogni precondizione $q$ dell'azione $\pi(s)$, $q \in s$
\end{center}
\begin{Nota}
    Qui le slide hanno degli artefatti??? La procedura potrebbe essere sbagliata!
\end{Nota}
L'euristica viene quindi impostata a $|\pi(s)|$, cioè il numero di azioni del piano rilassato.
Quest'euristica viene chiamata \textbf{piano rilassato} $h$ o $h_{FF}$; tuttavia, seppur sia \textbf{informativa} (è piuttosto accurata), essa \textbf{non è ammissibile}.
\subsection{Probabilistic Planning}
WIP
\section{Approcci simbolici: basi di conoscenza e ragionamento automatico}
WIP
\section{Approcci sub-simbolici: Machine Learning}
Due fattori importanti che hanno portato a importanti risultati nel campo dell'intelligenza artificiale sono:
\begin{itemize}
    \item \textbf{La crescita della potenza computazionale} dei dispositivi (specialmente le GPU e addirittura la nascita di dispositivi dedicati, es. TPUs)
    \item \textbf{La crescente disponibilità di dati} (specialmente foto, video ma anche testo in lingue differenti). In particolare, questo tipo di dati sono \textbf{annotati}, quindi contengono metadati che ne descrivono il contenuto.
\end{itemize} 
Le reti neurali non erano implementabili fino alla metà degli anni'80. Grazie al presentarsi delle condizioni sopra riportare, divenne quindi plausibile
\textbf{creare delle reti neurali profonde}. Un'esempio è \textbf{ImageNet}, un database di immagini annotate secondo la \textbf{gerarchia WorldNet}, la quale non è un'ontologia assiomatica, ma è un tesauro con relazioni sia \textbf{tassonomiche} che
\textbf{mereologiche} ("parte di"). ImageNet fu strumentale alla creazione delle prime reti neurali profonde.
\subsection{Introduzione al Machine Learning}
\begin{center}
    \includegraphics[width =0.95\linewidth]{Images/58.PNG}
\end{center}
\textbf{Machine Learning} è in realta un termine ombrello che racchiude sotto di se diversi tipi di apprendimento automatico.
Diamo una breve descrizione delle tecniche illustrate sopra:
\begin{itemize}
    \item \textbf{Apprendimento supervisionato}: Si sviluppa un modello predittivo basato sia sui dati in input che sui dati in output. Per andare ad analizzare dati non noti bisogna quindi avere una base esperienziale di dati noti; così che il modello possa associare un output plausibile al dato.
    \begin{itemize}
        \item \textbf{Classificazione}: Il modello riconosce l'appartenenza del dato ad una certa classe
        \item \textbf{Regressione}: Dati in input dei dati numerici, il modello darà in output un dato numerico che "si avvicina" al valore effettivo dato da quelle condizioni
    \end{itemize}
    \item \textbf{Apprendimento non supervisionato}: In questo tipo di apprendimento, il modello ha solamente il \textbf{dato in input}; esso quindi deve interpretare l'input e svolgere una mansione.
    \begin{itemize}
        \item \textbf{Clustering}: Il modello, analizzando i dati in input, trova dei raggruppamenti a cui le diverse parti dell'input appartengono
    \end{itemize} 
\end{itemize}
Tuttavia, come si può notare dallo schema, la situazione è ben più complessa rispetto all'elenco puntato sopra.
Inoltre, che relazione ha tutto questo con la statistica? Il deep learning dove si colloca in questo schema?
Facciamo quindi un passo indietro e chiediamoci la seguente domanda: qual'è la differenza tra la \textbf{programmazione tradizionale} e la \textbf{programmazione di modelli di apprendimento automatico}?
\begin{center}
    \includegraphics[width =0.65\linewidth]{Images/59.PNG}
\end{center}
\begin{itemize}
    \item \textbf{Programmare} è essenzialmente scrivere le operazioni che un esecutore automatico deve seguire per risolvere un qualche problema. Ciò significa che un programma tradizionale ha \textbf{come input dei dati} e darà in \textbf{output dei dati}.
    Tradizionalmente, un programmatore deve esplicitamente definire le operazioni che la macchina dovrà eseguire, cioè deve definire \textbf{le regole che l'esecutore dovrà seguire}.
    \item All'interno di alcune tecniche di apprendimento automatico, il programmatore \textbf{studia il problema} e seleziona un \textbf{algoritmo automatico} che, analizzando i dati, automaticamente produce un programma che risolverà il problema. Un \textbf{algoritmo di Machine Learning} quindi avrà in \textbf{input dei dati} e ritornerà in \textbf{output un programma}.
\end{itemize}
Ovviamente, entrambi i processi sono \textbf{iterativi}; ciò permette la gestione di errori e di risultati che non considerati "abbastanza buoni".
\subsubsection{Introduzione all'apprendimento supervisionato}
Nell'\textbf{apprendimento supervisionato}, i dati di addestramento che vengono dati in pasto all'algoritmo includono le \textbf{soluzioni desiderate}, chiamate \textbf{labels} (etichette).
\begin{itemize}
    \item Esse possono essere \textbf{categorie} (es. SPAM / NOT SPAM) nei \textbf{modelli di classificazione}. La supervisione, quindi, \textbf{è l'avere degli esempi con un'etichettatura}. Ovviamente, gli esempi devono essere in \textbf{numero sufficiente} e devono avere una \textbf{buona rappresentanza delle varia classi}.
    \item Oppure, possono essere \textbf{valori numerici}, dove si cerca di \textbf{apprendere una funzione non nota}, nei \textbf{modelli regressivi} 
\end{itemize}
\begin{center}
    \includegraphics[width =0.65\linewidth]{Images/60.PNG}
\end{center}
\subsubsection{Introduzione all'apprendimento non supervisionato}
Nell'\textbf{apprendimento non supervisionato}, i dati \textbf{non sono etichettati!}
Alcune tipiche mansioni in quest'area sono:
\begin{itemize}
    \item \textbf{Clustering}: Algoritmi che provano a individuare modi di \textbf{raggruppare istanze} all'interno del dataset secondo sia le loro \textbf{caratteristiche} sia le \textbf{caratteristiche dell'intero dataset}.
    \item \textbf{Dimensionality Reduction}: I dati possono essere caratterizzati da un numero molto elevato di caratteristiche; ciò li rende quindi molto difficili da gestire e visualizzare. Quindi, ha, a volte, senso ridurre il \textbf{numero di dimensioni} (cioè il numero di caratteristiche) \textbf{limitando la perdita di informazione}. Questo approccio è molto spesso \textbf{ancillare} ad altre tecniche.
    \item \textbf{Anomaly Detection}: I dati rappresentano un \textbf{singolo insieme di individui "regolari"} e il sistema deve imparare a discriminare i dati che appartengono a questo insieme da quelli non contenuti in esso, cioè dai dati che \textbf{non sono conformi ad una certa struttura dell'insieme}, essa stessa implicata dai dati.
\end{itemize}
\begin{center}
    \includegraphics[width =0.65\linewidth]{Images/61.PNG}
\end{center}
\begin{center}
    \includegraphics[width =0.50\linewidth]{Images/62.PNG}
\end{center}
\subsubsection{Altre classi di apprendimento automatico}
Altre classi di apprendimento automatico sono:
\begin{itemize}
    \item \textbf{Apprendimento semi-supervisionato}: Questo approccio gestisce \textbf{dati di addestramento parzialmente etichettati}. Solitamente, i dataset in cui viene impiegato questo approccio hanno \textbf{una mole molto elevata di dati non etichettati} e una \textbf{piccola quantità di dati etichettati}.
    Questo approccio tipicamente combina algoritmi supervisionati e non supervisionati.
    \item \textbf{Apprendimento per rinforzo}: Questo approccio è molto differente da quelli visti fino ad ora. In questo caso, l'agente viene posto all'interno di un ambiente, del quale conosce \textbf{quali azioni sono disponibili}, ed esso apprende \textbf{essenzialmente tramite un processo di "trial-and-error"}, visto che l'ambiente
    mette a sua disposizione delle \textbf{percezioni} e un \textbf{valore di ricompensa} associato alla precedente azione scelta dall'agente.
\end{itemize}
\subsubsection{Batch Learning vs Online Learning}
Un approccio di ML \textbf{non necessariamente migliora costantemente le proprie performance}.
Per esempio, la figura 1.2 di questa sezione \textbf{non descrive un caso in cui la soluzione "lanciata" viene revisionata}.
La revisione della soluzione dopo il lancio è \textbf{un approccio possibile}:
\begin{center}
    \includegraphics[width =0.65\linewidth]{Images/63.PNG}
\end{center}
tuttavia, l'aggiungere dati \textbf{non necessariamente porterà ad un aumento delle prestazioni del modello}.
Più precisamente, l'addestramento può avvenire secondo le seguenti modalità:
\begin{itemize}
    \item \textbf{Batch learning}: In questo approccio, il modello è addestrato su \textbf{tutti i dati al momento disponibili}
    \begin{itemize}
        \item Ciò, generalmente, richiede molto tempo e molte risorse computazionali, quindi è \textbf{tipicamente svolto offline}
    \end{itemize}
    \item \textbf{Online learning}: In questo approccio, il sistema viene addestrato dandogli in pasto \textbf{delle istanze di dati in maniera sequenziale}, o individualmente o in \textbf{piccoli gruppi}, detti "\textbf{mini-batches}". Gli algoritmi
    di online learning possono anche essere utilizzati per \textbf{addestrare modelli su dataset numerosi} che non riescono a rientrare all'interno della memoria principale di una singola macchina.
    \begin{itemize}
        \item È necessario inoltre definire \textbf{quanto velocemente il modello debba adattarsi alle modifiche nei dati} (\textbf{learning rate}) e quanto il dato nuovo sia rilevante.
    \end{itemize}
\end{itemize}
\begin{center}
    \includegraphics[width =0.70\linewidth]{Images/64.PNG}
\end{center}
\subsubsection{Necessità dell'addestramento di un modello}
Nel machine learning, \textbf{è sempre necessario che vi sia una complessa fase di addestramento del modello?}
La risposta è \textbf{no}. Nelle tecniche non supervisionate, è piuttosto frequente che \textbf{non vi sia una fase di addestramento}, ma semplicemente una fase di \textbf{analisi del dataset}.
Un approccio che non necessita di addestramento è quello del \textbf{instance-based learning}, il quale rappresenta un modo di \textbf{classificare dati basandosi sulla distanza dai dati vicini o sulla similarità}.
\begin{center}
    \includegraphics[width =0.75\linewidth]{Images/65.PNG}
\end{center}
\subsubsection{Criticità della valutazione dei modelli}
Nelle sezioni precedenti, abbiamo parlato di poter \textbf{dimostrare} delle affermazioni oppure di poter \textbf{trovare un valore ottimo}.
Negli approcci di ML, \textbf{è molto difficile che si sia in grado di dimostrare certe affermazioni}; diventa quindi \textbf{imperativo valutare i risultati di un modello},
quindi di discutere quanto essi siano "buoni" e quando e come falliscono. Tutti gli approcci visti fino ad ora hanno una base statistica e richiedono una valutazione dei risultati, le quali
non hanno necessariamente il \textbf{grado di forza che ha una dimostrazione} e quindi sono soggette sempre ad una costante verifica.
La valutazione dei risultati ci permette inoltre di \textbf{accorgerci di eventuali bias del modello}, dati anche da una possibile \textbf{sotto-rappresentazione di certe categorie}.
Le sfide quindi nell'ambito del ML sono:
\begin{itemize}
    \item Quantità di dati di addestramento insufficienti
    \item Dati di addestramento non rappresentativi
    \item Dati di scarsa qualità
    \item Presenza di caratteristiche irrilevanti
    \item Overfitting/Underfitting dei dati di addestramento
\end{itemize}
\begin{center}
    \includegraphics[width =0.80\linewidth]{Images/66.PNG}
\end{center}
\begin{center}
    \includegraphics[width =0.80\linewidth]{Images/67.PNG}
\end{center}
\newpage
\subsection{Apprendimento supervisionato}
Nell'apprendimento supervisionato abbiamo a disposizione una serie di \textbf{esempi} (record, casi, istanze, ecc...).
Diamo quindi delle definizioni più precise:
\begin{itemize}
    \item \textbf{Dati}: Una serie di "record" (anche chiamati esempi, istanze o casi) descritti da:
    \begin{itemize}
        \item \textbf{k attributi} $A_1, \dots, A_k$
        \item \textbf{Una classe} (se parliamo di classificatori): ogni esempio è etichettato con un classe definita a priori
    \end{itemize}
    \item \textbf{Obbiettivo}: Apprendere un \textbf{modello di classificazione} dai dati che possa essere usato per prevedere le classi di nuovi casi/istanze.
\end{itemize}
Se l'output $a$ non appartiene a un \textbf{insieme discreto}, ma è piuttosto un \textbf{valore numerico} (sia discreto che continuo), allora parliamo di \textbf{regressione invece che di classificazione}.
Il resto della discussione rimane invece praticamente uguale.
\begin{center}
    \includegraphics[width =0.80\linewidth]{Images/68.PNG}
\end{center}
Quali sono quindi le differenze con l'apprendimento non supervisionato?
\begin{itemize}
    \item \textbf{Apprendimento supervisionato}: La classificazione è vista come un apprendimento supervisionato dagli esempi
    \begin{itemize}
        \item \textbf{Supervisione}: I dati (osservazioni, misure, ecc...), sono etichettati con classi predefinite. È come se un "insegnate" dia le classi al modello (\textbf{supervisione})
        \item Anche i dati di test vengono classificati in queste classi
    \end{itemize}
    \item \textbf{Apprendimento non-supervisionato} (clustering): Le etichette di classe dei dati \textbf{non sono noti a priori}. Dato quindi un insieme di dati, il compito è quello di stabilire l'esistenza di classi o di \textbf{raggruppamenti} all'interno del dataset
\end{itemize}
\newpage
\noindent
Il flusso di lavoro dell'apprendimento supervisionato generalmente prevede le seguenti fasi:
\begin{enumerate}
    \item \textbf{Apprendimento (addestramento)}: Viene appreso un modello usando i \textbf{dati di addestramento}
    \item \textbf{Testing}: Si testa il modello utilizzato \textbf{dati di test non visti durante la fase di addestramento} per stabilire l'accuratezza del modello.
    Un modo per stimare l'accuratezza di un modello può essere la seguente:
    $$Accuracy = \frac{\textrm{Number of correct classifications}}{\textrm{Total number of test cases}}$$
\end{enumerate}
\begin{center}
    \includegraphics[width =0.80\linewidth]{Images/69.PNG}
\end{center}
È necessario notare che \textbf{i dati che vengono utilizzati per testare il modello non devono contenere dati che sono stati utilizzati per addestrarlo}.
L'assunzione di base che ci permette di adottare questo tipo di approccio è la seguente: \newline
\textbf{ASSUNZIONE}: La distribuzione degli esempi (quanti sono appartenenti alle diverse  classi? Ogni classe è rappresentata sufficientemente? E in che proporzione, ed essa è ragionevolmente vicina al mondo reale?) usati nell'addestramento è \textbf{identica} alla distribuzione dei casi di test (inclusi gli esempi futuri mai osservati prima). Cioè, che i dati
del dataset \textbf{siano rappresentativi di come sono davvero i dati nel mondo reale}. \newline
Nella pratica, questa assunzione è \textbf{spesso violata in una certa misura}. Ampie violazioni di questa assunzione, chiaramente, risulteranno in \textbf{una scarsa accuratezza di classificazione}.
Per raggiungere una buona accuratezza sui dati di test, i dati di addestramento devono essere \textbf{sufficientemente rappresentativi}.
\subsubsection{Induzione di alberi di decisione}
Gli alberi di decisione sono una tecnica nota ed utilizzata ai fini di \textbf{classificazione}:
\begin{itemize}
    \item Su dati tabellari, la loro accuratezza è molto competitiva con altri metodi di classificazione
    \item Sono molto efficienti
    \item Su altri tipi di dati, gli alberi di decisione invece risultano meno accurati o efficienti rispetto ad altri approcci
\end{itemize}
La tecnica che andremo a discutere \textbf{costruisce un albero}, il quale avrà sui nodi dei \textbf{test sugli attributi di un nuovo individuo} e che ci permetterà, navigando dalla radice alla foglia, di determinare la \textbf{classe da assegnare ad esso}.
La particolarità di questo albero è ispezionabile, leggibile è può essere \textbf{trasformato in regole del tipo if-then}. L'albero quindi è "spiegabile", cioè è interpretabile ed è possibile capire, da esso, come il modello effettua la classificazione. Non tutti gli algoritmi
producono un modello "spiegabile". Facciamo un esempio di albero di decisione usando la tabella sopra:
\begin{center}
    \includegraphics[width =0.80\linewidth]{Images/70.PNG}
\end{center}
i nodi dell'albero sono associati ad attributi della tabella. In base al tipo di dato, \textbf{il fattore di ramificazione può essere differente}.
Gli alberi di decisione, tuttavia, \textbf{non sono unici}; infatti un albero equivalente e più semplice rispetto a quello sopra è il seguente:
\begin{center}
    \includegraphics[width =0.40\linewidth]{Images/71.PNG}
\end{center}
vogliamo quindi trovare l'albero decisionale \textbf{più piccolo e più accurato possibile}, poiché esso avrà prestazioni migliori e risulterà più facile da comprendere.
Tuttavia, trovare un albero ottimale è un problema \textbf{NP-Completo}; quindi tutti i correnti algoritmi di costruzione di alberi decisionali sono \textbf{algoritmi euristici}; quindi sono algoritmi che non garantiscono l'ottimalità
ma hanno \textbf{ottime proprietà computazionali} (tempo di esecuzione e utilizzo di memoria). Per loro struttura, invece, fare un'inferenza su un albero decisionale ha un \textbf{costo logaritmico rispetto al numero dei nodi}, quindi effettuare una classificazione
tramite un albero decisionale ha un costo relativamente basso.
Come abbiamo detto, un albero di decisione può essere convertito in un \textbf{insieme di regole}; in particolare, ognuna di esse rappresenterà un \textbf{cammino dalla radice ad una foglia}.
Ogni regola sarà \textbf{supportata da un certo numero di casi} e avrà un indicatore di \textbf{quanto è numeroso il suo supporto}:
\begin{center}
    \includegraphics[width =0.90\linewidth]{Images/72.PNG}
\end{center}
Un \textbf{algoritmo semplice} per la costruzione di un albero decisionale è il seguente:
\begin{itemize}
    \item L'algoritmo è di tipo \textbf{greedy} ed adotta l'approccio \textbf{dividi-et-impera}
    \item Si assuma che gli attributi siano categorici (possono essere gestiti anche attributi continui)
    \item L'albero è costruito \textbf{ricorsivamente in maniera top-down}
    \item All'inizio, tutti gli esempi d'addestramento \textbf{sono posizionati alla radice}
    \item Gli esempi vengono poi \textbf{partizionati ricorsivamente} in base agli attributi selezionati
    \item La selezione degli attributi su cui effettuare la partizione viene effettuata sulla base di una \textbf{funzione di impurità}; che considera quanto è \textbf{eterogeneo il dataset rispetto ai risultati dato il valore di un certo attributo}
\end{itemize}
L'algoritmo terminerà la partizione del dataset nei seguenti casi:
\begin{itemize}
    \item Tutti gli esempi per un certo nodo appartengono alla stessa classe
    \item Non ci sono più attributi rimanenti per effettuare un'ulteriore partizione
    \item Non ci sono più esempi
\end{itemize}
Il codice dell'algoritmo è quindi il seguente:
\begin{center}
    \includegraphics[width =1\linewidth]{Images/73.PNG}
\end{center}
Facciamo delle considerazioni:
\begin{itemize}
    \item Iniziamo guardando i parametri della funzione:
    \begin{itemize}
        \item $D$ è l'insieme di individui in questo stadio dell'analisi dell'albero (inizialmente sarà l'intero insieme di addestramento, ma vedremo che sarà diviso in sottoinsiemi durante l'esecuzione)
        \item $A$ è l'insieme di attributi non ancora analizzati (inizialmente tutti, ma quando effettueremo una divisione del dataset, lo faremo basandoci sul valore di un certo attributo, il quale verrà poi rimosso da $A$)
        \item $T$ è la foglia che viene generata come \textbf{sostituto all'insieme degli individui rimanenti al livello corrente dell'analisi}. È quindi la \textbf{foglia che viene generata ad un certo livello dell'analisi del dataset}
    \end{itemize}
    \item Poiché l'algoritmo è ricorsivo, \textbf{esso conterrà una serie di casi base}:
    \begin{itemize}
        \item Il primo caso base è il caso in cui $D$ contiene solo casi in cui la classe è sempre la stessa. $D$ sarà quindi un "sottoinsieme puro", quindi $T$ sarà una foglia etichettata con la classe che caratterizza tutti gli elementi di $D$
        \item Il secondo caso base è quando \textbf{non vi sono più attributi da considerare}, quindi quando $A = \emptyset$. In questo caso, quindi, $T$ sarà una foglia etichettata con la classe che \textbf{occorre con più frequenza in $D$ a questo livello dell'analisi}
    \end{itemize}
    \item Se non si è in un caso base, allora significa che ci sono ancora attributi da analizzare e che vi è ancora \textbf{impurità nelle classi}, quindi vi è ancora \textbf{incertezza}. Devo quindi scegliere \textbf{rispetto a quale attributi, tra quelli rimasti, creare un nuovo sottoramo dell'albero}:
    \begin{itemize}
        \item Quale attributo scelgo?
        \item \textbf{Eccezione}: l'algoritmo sopra considera anche il caso nel quale l'attributo scelto porta ad una riduzione di impurità nel dataset rimanente minore rispetto ad una certa soglia. In questo caso, \textbf{l'algoritmo si comporta come nel secondo caso base}
    \end{itemize}
\end{itemize}
La chiave nella scelta dell'attributo rispetto al quale costruire un sottoramo dell'albero è la \textbf{riduzione dell'impurità nella parte del dataset considerata}, cioè \textbf{cioè la riduzione dell'incertezza}.
Nell'algoritmo sopra, la scelta si basa sul \textbf{guadagno informativo che si ottiene scegliendo un certo attributo}, concetto derivato dalla \textbf{teoria dell'informazione}. Facciamo un esempio: consideriamo i seguenti alberi decisionali equivalenti:
\begin{center}
    \includegraphics[width =1\linewidth]{Images/74.PNG}
\end{center}
l'albero (B) sembra essere \textbf{migliore} rispetto all'albero (A), poiché \textbf{ha un tasso minore di incertezza in ogni suo sotto-caso} ed, essendo più piccolo, è anche \textbf{computazionalmente migliore}.
% 46:51





\end{document}
